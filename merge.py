"""
åˆå¹¶è‚¡ç¥¨æ± ã€åŒèŠ±é¡ºè¡Œæƒ…æ•°æ®ã€æ¶¨åœæ•°æ®å’Œå¼‚åŠ¨æ•°æ®
"""
import os
import sys
import pandas as pd
import numpy as np
import time
from datetime import datetime, timedelta
from typing import Tuple, Optional
from stock_pool import StockPool
from wencai import WencaiUtils
from jygs import JygsUtils
from log_setup import get_logger
from trading_calendar import TradingCalendar
from notification import DingDingRobot

# é…ç½®å¸¸é‡
OUTPUT_BASE_DIR = './output'

# æ•°æ®åˆ—é…ç½®
MARKET_COLUMNS = ['code', 'market_code', 'çƒ­åº¦æ’å', 'ç«æ¢æ‰‹Z', 'ç«ä»·é‡‘é¢', 
                 'ç«ä»·æ¶¨å¹…', 'æ¶¨è·Œå¹…', 'å®ä½“æ¶¨å¹…', 'æ¢æ‰‹Z', 'æˆäº¤é¢', 'å¤§å•å‡€é¢', 'ä¸Šå¸‚æ¿å—', 'å‡ å¤©å‡ æ¿']

ZT_COLUMNS = ['code', 'market_code', 'æ¶¨åœæ—¥æœŸ', 'è¿æ¿', 'æ¶¨åœå°å•é¢', 
             'æ¶¨åœåŸå› ç±»åˆ«', 'å°æˆé‡æ¯”', 'å°æµé‡æ¯”']

JYGS_COLUMNS = ['code', 'å¼‚åŠ¨æ—¥æœŸ', 'çƒ­ç‚¹', 'çƒ­ç‚¹å¯¼ç«ç´¢', 'å¼‚åŠ¨åŸå› ', 'è§£æ']

# æ•°å€¼å‹åˆ—ï¼ˆéœ€è¦å¡«å……ä¸º0ï¼‰
NUMERIC_COLUMNS = ['å¸‚å€¼Z', 'æˆäº¤é¢', 'ç«ä»·é‡‘é¢', 'å¤§å•å‡€é¢', 'æ¶¨åœå°å•é¢', 
                  'çƒ­åº¦æ’å', 'ç«æ¢æ‰‹Z', 'è¿æ¿', 'å¼€æ¿æ¬¡æ•°', 'å°æˆé‡æ¯”', 'å°æµé‡æ¯”']

# å­—ç¬¦ä¸²å‹åˆ—ï¼ˆéœ€è¦å¡«å……ä¸ºé»˜è®¤å€¼ï¼‰
STRING_COLUMNS = ['çƒ­ç‚¹', 'çƒ­ç‚¹å¯¼ç«ç´¢', 'å¼‚åŠ¨åŸå› ', 'è§£æ']

# é‡‘é¢è½¬æ¢é…ç½®ï¼ˆå•ä½ï¼šå…ƒè½¬äº¿å…ƒï¼‰
AMOUNT_CONVERSION = {
    'å¸‚å€¼Z': (1e8, 0),      # è½¬æ¢ä¸ºäº¿å…ƒï¼Œä¿ç•™æ•´æ•°
    'æˆäº¤é¢': (1e8, 1),     # è½¬æ¢ä¸ºäº¿å…ƒï¼Œä¿ç•™1ä½å°æ•°
    'å¤§å•å‡€é¢': (1e8, 2),   # è½¬æ¢ä¸ºäº¿å…ƒï¼Œä¿ç•™2ä½å°æ•°
    'æ¶¨åœå°å•é¢': (1e8, 2), # è½¬æ¢ä¸ºäº¿å…ƒï¼Œä¿ç•™2ä½å°æ•°
    'ç«ä»·é‡‘é¢': (1e8, 2)    # è½¬æ¢ä¸ºäº¿å…ƒï¼Œä¿ç•™2ä½å°æ•°
}

# é…ç½®æ—¥å¿—
logger = get_logger("merge", "logs", "daily_research.log")

# å…¨å±€å¯¹è±¡
trading_calendar = TradingCalendar()
dingding_robot = DingDingRobot()


def _load_previous_data(date_str: str) -> Optional[pd.DataFrame]:
    """åŠ è½½å‰ä¸€äº¤æ˜“æ—¥æ•°æ®"""
    previous_date = trading_calendar.get_previous_trading_day(date_str)
    if not previous_date:
        return None
    
    previous_date_str = previous_date.strftime('%Y%m%d')
    previous_file = f"{OUTPUT_BASE_DIR}/{previous_date_str}/core_stocks.csv"
    
    if not os.path.exists(previous_file):
        logger.warning("æœªæ‰¾åˆ°å‰ä¸€äº¤æ˜“æ—¥çš„åˆå¹¶æ•°æ®æ–‡ä»¶ï¼Œä½¿ç”¨åŸå§‹è‚¡ç¥¨æ± æ•°æ®è¿›è¡Œå¯¹æ¯”")
        return None
    
    try:
        df = pd.read_csv(previous_file, dtype={'code': str, 'market_code': str})
        if 'code' in df.columns:
            df['code'] = df['code'].astype(str).str.replace("'", "")
        return df
    except Exception as e:
        logger.warning(f"è¯»å–å‰ä¸€äº¤æ˜“æ—¥æ•°æ®å¤±è´¥: {e}")
        return None

def _save_stock_changes(current_df: pd.DataFrame, previous_df: pd.DataFrame, 
                       new_codes: set, removed_codes: set, date_str: str):
    """ä¿å­˜æ–°å¢å’Œç§»é™¤çš„è‚¡ç¥¨æ•°æ®"""
    save_dir = f"output/{date_str}"
    os.makedirs(save_dir, exist_ok=True)
    
    # ä¿å­˜æ–°å¢è‚¡ç¥¨
    if new_codes:
        new_stocks_df = current_df[current_df['code'].astype(str).isin(new_codes)].copy()
        if 'é‡è¦åº¦' in new_stocks_df.columns:
            new_stocks_df = new_stocks_df.sort_values('é‡è¦åº¦', ascending=False)
        add_file_path = f"{save_dir}/add.csv"
        new_stocks_df.to_csv(add_file_path, index=False, encoding='utf-8-sig')
        logger.info(f"æ–°å¢è‚¡ç¥¨æ•°æ®ä¿å­˜: {add_file_path}, æ•°é‡: {len(new_stocks_df)}")
    
    # ä¿å­˜ç§»é™¤è‚¡ç¥¨
    if removed_codes:
        removed_stocks_df = previous_df[previous_df['code'].astype(str).isin(removed_codes)].copy()
        if 'é‡è¦åº¦' in removed_stocks_df.columns:
            removed_stocks_df = removed_stocks_df.sort_values('é‡è¦åº¦', ascending=False)
        remove_file_path = f"{save_dir}/remove.csv"
        removed_stocks_df.to_csv(remove_file_path, index=False, encoding='utf-8-sig')
        logger.info(f"ç§»é™¤è‚¡ç¥¨æ•°æ®ä¿å­˜: {remove_file_path}, æ•°é‡: {len(removed_stocks_df)}")

def _build_comparison_message(current_df: pd.DataFrame, previous_df: pd.DataFrame,
                            new_codes: set, removed_codes: set, previous_date) -> str:
    """æ„å»ºå¯¹æ¯”æ¶ˆæ¯"""
    current_codes = set(current_df['code'].astype(str))
    previous_codes = set(previous_df['code'].astype(str))
    
    message = [
        "ğŸ“Š æ¯æ—¥å¤ç›˜:è‚¡ç¥¨æ± ",
        f"ğŸ“… å¯¹æ¯”åŸºå‡†: {previous_date.strftime('%Y-%m-%d')}",
        f"ğŸ“ˆ ä»Šæ—¥: {len(current_codes)}åª | æ˜¨æ—¥: {len(previous_codes)}åª"
    ]
    
    if not new_codes and not removed_codes:
        message.append("âœ¨ è‚¡ç¥¨æ± æ— å˜åŠ¨")
        return "\n".join(message)
    
    # æ–°å¢è‚¡ç¥¨è¯¦æƒ…
    if new_codes:
        message.append(f"ğŸ†• æ–°å¢: {len(new_codes)}åª")
        new_stocks_info = current_df[current_df['code'].astype(str).isin(new_codes)].copy()
        if 'é‡è¦åº¦' in new_stocks_info.columns:
            new_stocks_info = new_stocks_info.sort_values('é‡è¦åº¦', ascending=False)
        
        for _, stock in new_stocks_info.iterrows():
            code = stock['code']
            name = stock.get('è‚¡ç¥¨ç®€ç§°', 'æœªçŸ¥')
            interval_info = stock.get('åŒºé—´ä¿¡æ¯', 'æ— ')
            hotspot = stock.get('çƒ­ç‚¹', 'å…¶ä»–')
            message.append(f"  â€¢ {name}({code}) çƒ­ç‚¹:{hotspot} åŒºé—´:{interval_info}")
    
    # ç§»é™¤è‚¡ç¥¨è¯¦æƒ…
    if removed_codes:
        message.append(f"ğŸ”» ç§»é™¤: {len(removed_codes)}åª")
        removed_stocks_info = previous_df[previous_df['code'].astype(str).isin(removed_codes)].copy()
        if 'é‡è¦åº¦' in removed_stocks_info.columns:
            removed_stocks_info = removed_stocks_info.sort_values('é‡è¦åº¦', ascending=False)
        
        for _, stock in removed_stocks_info.iterrows():
            code = stock['code']
            name = stock.get('è‚¡ç¥¨ç®€ç§°', 'æœªçŸ¥')
            interval_info = stock.get('åŒºé—´ä¿¡æ¯', 'æ— ')
            hotspot = stock.get('çƒ­ç‚¹', 'å…¶ä»–')
            message.append(f"  â€¢ {name}({code}) çƒ­ç‚¹:{hotspot} åŒºé—´:{interval_info}")
    
    return "\n".join(message)

def compare_previous(current_merged_df: pd.DataFrame, date_str: str = None) -> str:
    """ä¸å‰ä¸€ä¸ªäº¤æ˜“æ—¥çš„åˆå¹¶åæ•°æ®è¿›è¡Œå¯¹æ¯”"""
    if date_str is None:
        date_str = trading_calendar.get_default_trade_date()
    
    # åŠ è½½å‰ä¸€äº¤æ˜“æ—¥æ•°æ®
    previous_df = _load_previous_data(date_str)
    if previous_df is None:
        return f"ğŸ“Š ä»Šæ—¥è‚¡ç¥¨æ± æ›´æ–°å®Œæˆ\næ•°æ®é‡: {len(current_merged_df)}åªè‚¡ç¥¨\nâš ï¸ æœªæ‰¾åˆ°å‰ä¸€äº¤æ˜“æ—¥æ•°æ®"
    
    # æ¸…ç†å½“å‰æ•°æ®çš„codeåˆ—
    current_df_clean = current_merged_df.copy()
    if 'code' in current_df_clean.columns:
        current_df_clean['code'] = current_df_clean['code'].astype(str).str.replace("'", "")
    
    # è®¡ç®—è‚¡ç¥¨æ± å˜åŠ¨
    current_codes = set(current_df_clean['code'].astype(str))
    previous_codes = set(previous_df['code'].astype(str))
    new_codes = current_codes - previous_codes
    removed_codes = previous_codes - current_codes
    
    # ä¿å­˜å˜åŠ¨æ•°æ®
    _save_stock_changes(current_df_clean, previous_df, new_codes, removed_codes, date_str)
    
    # æ„å»ºå¹¶è¿”å›å¯¹æ¯”æ¶ˆæ¯
    previous_date = trading_calendar.get_previous_trading_day(date_str)
    msg = _build_comparison_message(current_df_clean, previous_df, new_codes, removed_codes, previous_date)
    logger.info(msg)
    return msg

def load_all_data() -> Optional[Tuple]:
    """
    åŠ è½½æ‰€æœ‰å¸‚åœºæ•°æ®ï¼ŒåŒ…æ‹¬è‚¡ç¥¨æ± ã€è¡Œæƒ…æ•°æ®ã€æ¶¨åœæ•°æ®å’Œå¼‚åŠ¨æ•°æ®
    
    Returns:
        Optional[Tuple]: åŒ…å«æ‰€æœ‰æ•°æ®DataFrameçš„å…ƒç»„ï¼Œå¦‚æœåŠ è½½å¤±è´¥åˆ™è¿”å›None
            - core_stocks_data: æ ¸å¿ƒè‚¡ç¥¨æ± æ•°æ®
            - added_stocks_data: æ–°å¢è‚¡ç¥¨æ± æ•°æ®  
            - removed_stocks_data: å‡å°‘è‚¡ç¥¨æ± æ•°æ®
            - first_board_stocks_data: é¦–æ¿è‚¡ç¥¨æ± æ•°æ®
            - market_overview_data: åŒèŠ±é¡ºè¡Œæƒ…æ¦‚è§ˆæ•°æ®
            - zt_stocks_data: æ¶¨åœè‚¡ç¥¨æ•°æ®
            - jygs_data: å¼‚åŠ¨è‚¡ç¥¨æ•°æ®
    """
    try:
        logger.info("å¼€å§‹åŠ è½½å¸‚åœºæ•°æ®...")
        
        # åˆ›å»ºè‚¡ç¥¨æ± å®ä¾‹ï¼Œé¿å…é‡å¤åˆ›å»º
        stock_pool_manager = StockPool()
        
        logger.info("1.æ­£åœ¨è¯»å–æ ¸å¿ƒè‚¡ç¥¨æ± æ•°æ®...")
        core_stocks_data = stock_pool_manager.read_stock_pool_data(prefix='core_stocks')
        if core_stocks_data is None or core_stocks_data.empty:
            logger.error("æ ¸å¿ƒè‚¡ç¥¨æ± æ•°æ®ä¸ºç©º")
            raise Exception("æ ¸å¿ƒè‚¡ç¥¨æ± æ•°æ®ä¸ºç©º")
        logger.info(f"æ ¸å¿ƒè‚¡ç¥¨æ± æ•°æ®é‡: {len(core_stocks_data)}")

        logger.info("2.æ­£åœ¨è¯»å–é¦–æ¿è‚¡ç¥¨æ± æ•°æ®...")
        first_board_stocks_data = stock_pool_manager.read_stock_pool_data(prefix='first_stocks')
        if first_board_stocks_data is None or first_board_stocks_data.empty:
            logger.error("é¦–æ¿è‚¡ç¥¨æ± æ•°æ®ä¸ºç©º")
            raise Exception("é¦–æ¿è‚¡ç¥¨æ± æ•°æ®ä¸ºç©º")
        logger.info(f"é¦–æ¿è‚¡ç¥¨æ± æ•°æ®é‡: {len(first_board_stocks_data)}")
        
        logger.info("3. æ­£åœ¨è¯»å–å¸‚åœºè¡Œæƒ…æ•°æ®...")
        market_overview_data = WencaiUtils.read_market_overview_data()
        if market_overview_data is None or market_overview_data.empty:
            logger.error("å¸‚åœºè¡Œæƒ…æ•°æ®ä¸ºç©º")
            raise Exception("å¸‚åœºè¡Œæƒ…æ•°æ®ä¸ºç©º")
        logger.info(f"å¸‚åœºè¡Œæƒ…æ•°æ®é‡: {len(market_overview_data)}")
        
        logger.info("4.æ­£åœ¨è¯»å–æ›´æ–°åçš„æ¶¨åœæ•°æ®...")
        zt_stocks_data = WencaiUtils.read_latest_zt_stocks()
        if zt_stocks_data is None or zt_stocks_data.empty:
            logger.error("æ¶¨åœè‚¡ç¥¨æ•°æ®ä¸ºç©º")
            raise Exception("æ¶¨åœè‚¡ç¥¨æ•°æ®ä¸ºç©º")
        logger.info(f"æ¶¨åœè‚¡ç¥¨æ•°æ®é‡: {len(zt_stocks_data)}")
        zt_stocks_data.rename(columns={'äº¤æ˜“æ—¥æœŸ': 'æ¶¨åœæ—¥æœŸ'}, inplace=True)
        
        logger.info("5.æ­£åœ¨è¯»å–æ›´æ–°åçš„å¼‚åŠ¨æ•°æ®...")
        jygs_data = JygsUtils.read_stocks_data()
        if jygs_data is None or jygs_data.empty:
            logger.error("å¼‚åŠ¨è‚¡ç¥¨æ•°æ®ä¸ºç©º")
            raise Exception("å¼‚åŠ¨è‚¡ç¥¨æ•°æ®ä¸ºç©º")
        logger.info(f"å¼‚åŠ¨è‚¡ç¥¨æ•°æ®é‡: {len(jygs_data)}")
        jygs_data.rename(columns={'äº¤æ˜“æ—¥æœŸ': 'å¼‚åŠ¨æ—¥æœŸ'}, inplace=True)
        
        logger.info("æ‰€æœ‰å¿…è¦çš„æ•°æ®åŠ è½½å®Œæˆ")
        return (core_stocks_data, 
                first_board_stocks_data, market_overview_data, 
                zt_stocks_data, jygs_data)
                
    except Exception as e:
        logger.error(f"åŠ è½½å¸‚åœºæ•°æ®å¤±è´¥: {e}")
        raise

def clean_data(merged_data: pd.DataFrame) -> pd.DataFrame:
    """æ¸…æ´—å’Œæ ¼å¼åŒ–è´¢åŠ¡æ•°æ®"""
    try:
        # å¤„ç†æ•°å€¼å‹åˆ—
        for column in NUMERIC_COLUMNS:
            if column in merged_data.columns:
                merged_data[column] = merged_data[column].replace([np.inf, -np.inf], np.nan)
                merged_data[column].fillna(0, inplace=True)
        
        # å¤„ç†å­—ç¬¦ä¸²å‹åˆ—
        for column in STRING_COLUMNS:
            if column in merged_data.columns:
                merged_data[column].fillna('å…¶ä»–', inplace=True)
        
        # é‡‘é¢å•ä½è½¬æ¢ï¼ˆå…ƒè½¬äº¿å…ƒï¼‰
        for column, (divisor, decimal_places) in AMOUNT_CONVERSION.items():
            if column in merged_data.columns:
                try:
                    merged_data[column] = merged_data[column].apply(
                        lambda x: round(float(x) / divisor, decimal_places) if pd.notnull(x) else 0
                    )
                except Exception as e:
                    logger.error(f"è½¬æ¢{column}åˆ—æ—¶å‘ç”Ÿé”™è¯¯: {e}")
                    continue
        
        return merged_data
        
    except Exception as e:
        logger.error(f"æ¸…æ´—å’Œæ ¼å¼åŒ–è´¢åŠ¡æ•°æ®æ—¶å‘ç”Ÿé”™è¯¯: {e}")
        return merged_data


class MarketData:
    """å¸‚åœºæ•°æ®å°è£…ç±»"""
    def __init__(self, market_overview: pd.DataFrame, zt_stocks: pd.DataFrame, jygs: pd.DataFrame):
        self.market_overview = market_overview
        self.zt_stocks = zt_stocks
        self.jygs = jygs

def _get_available_columns(columns: list, df: pd.DataFrame) -> list:
    """è·å–DataFrameä¸­å®é™…å­˜åœ¨çš„åˆ—"""
    return [col for col in columns if col in df.columns]

def _merge_market_data(stock_pool_data: pd.DataFrame, market_data: MarketData) -> pd.DataFrame:
    """åˆå¹¶å¸‚åœºæ•°æ®"""
    # é€æ­¥åˆå¹¶æ•°æ®
    merged_data = pd.merge(
        stock_pool_data, 
        market_data.market_overview[_get_available_columns(MARKET_COLUMNS, market_data.market_overview)], 
        on=['code', 'market_code'], 
        how='left'
    )
    
    merged_data = pd.merge(
        merged_data, 
        market_data.zt_stocks[_get_available_columns(ZT_COLUMNS, market_data.zt_stocks)], 
        on=['code', 'market_code'], 
        how='left'
    )
    
    merged_data = pd.merge(
        merged_data, 
        market_data.jygs[_get_available_columns(JYGS_COLUMNS, market_data.jygs)], 
        on=['code'], 
        how='left'
    )
    
    return merged_data

def _format_data_columns(merged_data: pd.DataFrame) -> pd.DataFrame:
    """æ ¼å¼åŒ–æ•°æ®åˆ—"""
    # æ ¼å¼åŒ–ä»£ç å’ŒåŒºé—´ä¿¡æ¯ï¼ˆæ·»åŠ å•å¼•å·å‰ç¼€ï¼‰
    if 'code' in merged_data.columns and not merged_data.empty:
        merged_data['code'] = merged_data['code'].apply(lambda x: f"'{x}")
    
    if 'åŒºé—´ä¿¡æ¯' in merged_data.columns and not merged_data.empty:
        merged_data['åŒºé—´ä¿¡æ¯'] = merged_data['åŒºé—´ä¿¡æ¯'].apply(lambda x: f"'{x}")
    
    # åˆ é™¤ä¸éœ€è¦çš„åˆ—
    if 'market_code' in merged_data.columns:
        merged_data.drop(columns=['market_code'], inplace=True)
    
    return merged_data

def merge_data(stock_pool_data: pd.DataFrame, 
               market_data: MarketData,
               output_prefix: str = 'core_stocks',
               date_str: str = None) -> Optional[pd.DataFrame]:
    """å°†è‚¡ç¥¨æ± æ•°æ®ä¸å¸‚åœºæ•°æ®è¿›è¡Œåˆå¹¶"""
    try:
        if date_str is None:
            date_str = trading_calendar.get_default_trade_date()
        
        logger.info(f"å¼€å§‹åˆå¹¶{output_prefix}æ•°æ®...")
        
        # éªŒè¯è¾“å…¥æ•°æ®    
        if not all([
            stock_pool_data is not None and not stock_pool_data.empty,
            market_data.market_overview is not None and not market_data.market_overview.empty,
            market_data.zt_stocks is not None and not market_data.zt_stocks.empty,
            market_data.jygs is not None and not market_data.jygs.empty
        ]):
            logger.error("è¾“å…¥æ•°æ®éªŒè¯å¤±è´¥ï¼Œæ— æ³•è¿›è¡Œåˆå¹¶")
            return None
        
        # å¤„ç†ç©ºè‚¡ç¥¨æ± çš„æƒ…å†µ
        if stock_pool_data.empty:
            logger.info(f"{output_prefix}è‚¡ç¥¨æ± ä¸ºç©ºï¼Œåˆ›å»ºç©ºçš„åˆå¹¶ç»“æœ")
            merged_data = pd.DataFrame(columns=['code'])
        else:
            merged_data = _merge_market_data(stock_pool_data, market_data)
        
        # æ ¼å¼åŒ–æ•°æ®åˆ—
        merged_data = _format_data_columns(merged_data)
        
        # æ¸…æ´—å’Œæ ¼å¼åŒ–è´¢åŠ¡æ•°æ®
        merged_data = clean_data(merged_data)
        
        # ä¿å­˜åˆå¹¶åçš„æ•°æ®
        output_dir = f"{OUTPUT_BASE_DIR}/{date_str}"
        os.makedirs(output_dir, exist_ok=True)
        output_path = f'{output_dir}/{output_prefix}.csv'
        merged_data.to_csv(output_path, index=False, encoding='utf-8-sig')
        logger.info(f"{output_prefix}æ•°æ®åˆå¹¶å®Œæˆï¼Œå·²ä¿å­˜åˆ°: {output_path}")
        
        return merged_data
        
    except Exception as e:
        logger.error(f"åˆå¹¶{output_prefix}æ•°æ®å¤±è´¥: {e}")
        return None



def identify_emerging_hotspots(date_str: str = None, lookback_days: int = 10) -> list:
    """
    è¯†åˆ«ä»Šæ—¥æ–°å‡ºç°çš„çƒ­ç‚¹
    
    Args:
        lookback_days: å›çœ‹çš„äº¤æ˜“æ—¥å¤©æ•°ï¼Œé»˜è®¤10å¤©
        
    Returns:
        list: ä»Šæ—¥æ–°å‡ºç°çš„çƒ­ç‚¹åˆ—è¡¨
    """
    def is_hotspot_similar(hotspot_a: str, hotspot_b: str) -> bool:
        return (hotspot_a in hotspot_b) or (hotspot_b in hotspot_a)
    
    try:
        # è¯»å–æ¿å—å†å²æ•°æ®
        bk_historical_data = JygsUtils.read_bk_data()
        if bk_historical_data.empty:
            logger.error("æ¿å—å†å²æ•°æ®ä¸ºç©º")
            raise Exception("æ¿å—å†å²æ•°æ®ä¸ºç©º")
            
        trading_dates = bk_historical_data['äº¤æ˜“æ—¥æœŸ'].unique()
        if len(trading_dates) == 0 or date_str != trading_dates[0]:
            logger.error("æ²¡æœ‰æ‰¾åˆ°äº¤æ˜“æ—¥æœŸçš„æ¿å—æ•°æ®")
            raise Exception("æ²¡æœ‰æ‰¾åˆ°äº¤æ˜“æ—¥æœŸçš„æ¿å—æ•°æ®")
        
        # è·å–è¿‡å»Nä¸ªäº¤æ˜“æ—¥çš„æ—¥æœŸèŒƒå›´
        past_trading_dates = trading_dates[1:lookback_days+1] if len(trading_dates) > lookback_days else trading_dates[1:]
        
        # æå–è¿‡å»Nå¤©çš„æ‰€æœ‰çƒ­ç‚¹
        past_hotspots = bk_historical_data[bk_historical_data['äº¤æ˜“æ—¥æœŸ'].isin(past_trading_dates)]['çƒ­ç‚¹'].unique()
        # è·å–ä»Šæ—¥çƒ­ç‚¹æ•°æ®
        latest_trading_date = trading_dates[0]
        today_data = bk_historical_data[bk_historical_data['äº¤æ˜“æ—¥æœŸ'] == latest_trading_date]
        today_hotspots = today_data['çƒ­ç‚¹'].unique()
        # ç­›é€‰æ–°å…´çƒ­ç‚¹
        emerging_hotspots = []
        for today_hotspot in today_hotspots:
            is_existing = False
            for past_hotspot in past_hotspots:
                if is_hotspot_similar(today_hotspot, past_hotspot):
                    is_existing = True
                    break
            
            if not is_existing and today_hotspot != 'å…¶ä»–':
                emerging_hotspots.append(today_hotspot)
        
        logger.info(f"è¯†åˆ«åˆ° {len(emerging_hotspots)} ä¸ªæ–°å…´çƒ­ç‚¹: {emerging_hotspots}")
        return emerging_hotspots
        
    except Exception as e:
        logger.error(f"è¯†åˆ«æ–°å…´çƒ­ç‚¹æ—¶å‘ç”Ÿé”™è¯¯: {e}")
        raise



def _format_stock_info(stock: pd.Series) -> str:
    """æ ¼å¼åŒ–è‚¡ç¥¨ä¿¡æ¯"""
    code = str(stock['code']).replace("'", "")
    name = stock.get('è‚¡ç¥¨ç®€ç§°', 'æœªçŸ¥')
    jygs_reason = stock.get('å¼‚åŠ¨åŸå› ', 'å…¶ä»–')
    hotspot_trigger = stock.get('çƒ­ç‚¹å¯¼ç«ç´¢', 'æ— ')
    
    stock_info = f"  â€¢ {name}({code})"
    if jygs_reason and jygs_reason != 'å…¶ä»–':
        stock_info += f" | å¼‚åŠ¨åŸå› : {jygs_reason}"
    if hotspot_trigger and hotspot_trigger != 'æ— ':
        stock_info += f" | å¯¼ç«ç´¢: {hotspot_trigger}"
    
    return stock_info

def generate_report(merged_first_board_data: pd.DataFrame, 
                   emerging_hotspots: list,
                   date_str: str = None) -> str:
    """ç”Ÿæˆæ–°å…´çƒ­ç‚¹æŠ¥å‘Š"""
    if date_str is None:
        date_str = trading_calendar.get_default_trade_date()
    
    if not emerging_hotspots:
        return "ğŸ“Š ä»Šæ—¥æ–°å…´çƒ­ç‚¹åˆ†æ\nâœ¨ æš‚æ— æ–°å‡ºç°çš„çƒ­ç‚¹"
    
    try:
        # ç­›é€‰æ–°å…´çƒ­ç‚¹ç›¸å…³çš„è‚¡ç¥¨
        emerging_hotspots_df = merged_first_board_data[
            merged_first_board_data['çƒ­ç‚¹'].isin(emerging_hotspots)
        ].copy()
        
        if emerging_hotspots_df.empty:
            return f"ğŸ“Š ä»Šæ—¥æ–°å…´çƒ­ç‚¹åˆ†æ\nğŸ” å‘ç°{len(emerging_hotspots)}ä¸ªæ–°çƒ­ç‚¹ï¼Œä½†æ— ç›¸å…³é¦–æ¿è‚¡ç¥¨æ•°æ®"
        
        # ä¿å­˜æ–°å…´çƒ­ç‚¹æ•°æ®
        save_dir = f"output/{date_str}"
        os.makedirs(save_dir, exist_ok=True)
        emerging_hotspots_file = f"{save_dir}/emerging_hotspots.csv"
        emerging_hotspots_df.to_csv(emerging_hotspots_file, index=False, encoding='utf-8-sig')
        logger.info(f"æ–°å…´çƒ­ç‚¹æ•°æ®ä¿å­˜: {emerging_hotspots_file}, æ•°é‡: {len(emerging_hotspots_df)}")
        
        # æ„å»ºæ ¼å¼åŒ–æ¶ˆæ¯
        message = [
            "ğŸ“Š ä»Šæ—¥æ–°å…´çƒ­ç‚¹åˆ†æ",
            f"ğŸ”¥ å‘ç°{len(emerging_hotspots)}ä¸ªæ–°çƒ­ç‚¹ï¼Œæ¶‰åŠ{len(emerging_hotspots_df)}åªé¦–æ¿è‚¡ç¥¨",
            ""
        ]
        
        # æŒ‰çƒ­ç‚¹åˆ†ç»„å±•ç¤ºè‚¡ç¥¨ä¿¡æ¯
        for hotspot in emerging_hotspots:
            hotspot_stocks = emerging_hotspots_df[emerging_hotspots_df['çƒ­ç‚¹'] == hotspot]
            if not hotspot_stocks.empty:
                message.append(f"ğŸ¯ çƒ­ç‚¹: {hotspot}")
                
                # æŒ‰é‡è¦åº¦æ’åº
                if 'é‡è¦åº¦' in hotspot_stocks.columns:
                    hotspot_stocks = hotspot_stocks.sort_values('é‡è¦åº¦', ascending=False)
                
                for _, stock in hotspot_stocks.iterrows():
                    message.append(_format_stock_info(stock))
                
                message.append("")  # æ·»åŠ ç©ºè¡Œåˆ†éš”
        
        msg = "\n".join(message)
        logger.info(msg)
        return msg
        
    except Exception as e:
        logger.error(f"ç”Ÿæˆæ–°å…´çƒ­ç‚¹æŠ¥å‘Šæ—¶å‘ç”Ÿé”™è¯¯: {e}")
        return f"ğŸ“Š ä»Šæ—¥æ–°å…´çƒ­ç‚¹åˆ†æ\nâŒ ç”ŸæˆæŠ¥å‘Šæ—¶å‘ç”Ÿé”™è¯¯: {e}"


def merge():
    """åˆå¹¶è‚¡ç¥¨æ± æ•°æ®ã€å¸‚åœºæ•°æ®ã€æ¶¨åœæ•°æ®å’Œå¼‚åŠ¨æ•°æ®"""
    try:
        logger.info("å¼€å§‹æ‰§è¡Œåˆå¹¶æ•°æ®ä¸»æµç¨‹")
        
        # è·å–å½“å‰äº¤æ˜“æ—¥æœŸ
        current_date = trading_calendar.get_default_trade_date()
        logger.info(f"å½“å‰äº¤æ˜“æ—¥æœŸ: {current_date}")
        
        # æ­¥éª¤1: åŠ è½½æ‰€æœ‰æ•°æ®
        logger.info("åŠ è½½è‚¡ç¥¨æ± å’Œå¸‚åœºæ•°æ®...")
        loaded_data = load_all_data()
        (core_stocks_data, first_board_stocks_data, 
         market_overview_data, zt_stocks_data, jygs_data) = loaded_data
        market_data = MarketData(market_overview_data, zt_stocks_data, jygs_data)

        # æ­¥éª¤2: åˆå¹¶æ ¸å¿ƒè‚¡ç¥¨æ± æ•°æ®
        logger.info("åˆå¹¶æ ¸å¿ƒè‚¡ç¥¨æ± æ•°æ®...")
        merged_core_data = merge_data(core_stocks_data, market_data, 'core_stocks', current_date)
        if merged_core_data is None:
            raise Exception("æ ¸å¿ƒè‚¡ç¥¨æ± æ•°æ®åˆå¹¶å¤±è´¥")

        # æ­¥éª¤3: æ ¸å¿ƒè‚¡ç¥¨æ± å¯¹æ¯”åˆ†æ
        logger.info("æ ¸å¿ƒè‚¡ç¥¨æ± å¯¹æ¯”åˆ†æ...")
        comparison_msg = compare_previous(merged_core_data, current_date)
        dingding_robot.send_message(comparison_msg, 'robot3')

        # æ­¥éª¤4: åˆå¹¶é¦–æ¿è‚¡ç¥¨æ± æ•°æ®
        logger.info("åˆå¹¶é¦–æ¿è‚¡ç¥¨æ± æ•°æ®...")
        merged_first_board_data = merge_data(first_board_stocks_data, market_data, 'first_stocks', current_date)
        if merged_first_board_data is None:
            raise Exception("é¦–æ¿è‚¡ç¥¨æ± æ•°æ®åˆå¹¶å¤±è´¥")

        # æ­¥éª¤5: è¯†åˆ«æ–°å…´çƒ­ç‚¹å¹¶ç”ŸæˆæŠ¥å‘Š
        logger.info("è¯†åˆ«æ–°å…´çƒ­ç‚¹...")
        emerging_hotspots = identify_emerging_hotspots(date_str=current_date)
        hotspots_report_msg = generate_report(merged_first_board_data, emerging_hotspots, current_date)
        dingding_robot.send_message(hotspots_report_msg, 'robot3')

        logger.info("åˆå¹¶æ•°æ®ä¸»æµç¨‹æ‰§è¡Œå®Œæˆ")
    
    except Exception as e:
        logger.error(f"ç¨‹åºæ‰§è¡Œå¤±è´¥: {e}")
        dingding_robot.send_message(f"æ¯æ—¥æ•°æ®å¤„ç†å¤±è´¥: {e}", 'robot3')
        raise


if __name__ == "__main__":
    merge()
