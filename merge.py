"""
åˆå¹¶è‚¡ç¥¨æ± ã€åŒèŠ±é¡ºè¡Œæƒ…æ•°æ®ã€æ¶¨åœæ•°æ®å’Œå¼‚åŠ¨æ•°æ®
"""
import os
import sys
import pandas as pd
import numpy as np
import time
from datetime import datetime, timedelta
from typing import Tuple, Optional
from stock_pool import StockPool
from wencai import WencaiUtils
from jygs import JygsUtils
from log_setup import get_logger
from trading_calendar import TradingCalendar
from notification import DingDingRobot

# é…ç½®å¸¸é‡
OUTPUT_DIR = './data/csv'
OUTPUT_FILE_PREFIX = 'merge_'

# é…ç½®æ—¥å¿—
logger = get_logger("merge", "logs", "daily_research.log")

# å…¨å±€å¯¹è±¡
trading_calendar = TradingCalendar()
dingding_robot = DingDingRobot()


def compare_previous(current_merged_df: pd.DataFrame, date_str: str = None) -> str:
    """
    ä¸å‰ä¸€ä¸ªäº¤æ˜“æ—¥çš„åˆå¹¶åæ•°æ®è¿›è¡Œå¯¹æ¯”ï¼Œè¿”å›å¯¹æ¯”æ¶ˆæ¯ï¼Œå¹¶ä¿å­˜æ–°å¢å’Œå‡å°‘çš„è‚¡ç¥¨å®½è¡¨æ•°æ®
    
    Args:
        current_merged_df: å½“å‰åˆå¹¶åçš„å®½è¡¨æ•°æ®
        date_str: æ—¥æœŸå­—ç¬¦ä¸²ï¼Œé»˜è®¤ä¸ºNoneæ—¶ä½¿ç”¨å½“å‰äº¤æ˜“æ—¥
        
    Returns:
        str: å¯¹æ¯”æ¶ˆæ¯
    """
    if date_str is None:
        date_str = trading_calendar.get_default_trade_date()
    
    # è·å–å‰ä¸€äº¤æ˜“æ—¥æ•°æ®
    previous_date = trading_calendar.get_previous_trading_day(date_str)
    if not previous_date:
        return f"ğŸ“Š ä»Šæ—¥è‚¡ç¥¨æ± æ›´æ–°å®Œæˆ\næ•°æ®é‡: {len(current_merged_df)}åªè‚¡ç¥¨\nâš ï¸ æœªæ‰¾åˆ°å‰ä¸€äº¤æ˜“æ—¥æ•°æ®"
    
    # æŸ¥æ‰¾å‰ä¸€äº¤æ˜“æ—¥çš„åˆå¹¶åCSVæ–‡ä»¶
    previous_date_str = previous_date.strftime('%Y%m%d')
    previous_file = f"data/csv/merge_core_stocks_{previous_date_str}.csv"
    
    if not os.path.exists(previous_file):
        logger.warning("æœªæ‰¾åˆ°å‰ä¸€äº¤æ˜“æ—¥çš„åˆå¹¶æ•°æ®æ–‡ä»¶ï¼Œä½¿ç”¨åŸå§‹è‚¡ç¥¨æ± æ•°æ®è¿›è¡Œå¯¹æ¯”")
        # å¦‚æœæ²¡æœ‰åˆå¹¶æ•°æ®ï¼Œå°è¯•ä½¿ç”¨åŸå§‹è‚¡ç¥¨æ± æ•°æ®
        previous_raw_file = f"data/csv/stock_pool/core_stocks_{previous_date_str}.csv"
        if not os.path.exists(previous_raw_file):
            return f"ğŸ“Š ä»Šæ—¥è‚¡ç¥¨æ± æ›´æ–°å®Œæˆ\næ•°æ®é‡: {len(current_merged_df)}åªè‚¡ç¥¨\nâš ï¸ æœªæ‰¾åˆ°å‰ä¸€äº¤æ˜“æ—¥æ•°æ®æ–‡ä»¶"
        previous_file = previous_raw_file
    
    try:
        previous_df = pd.read_csv(previous_file, dtype={'code': str, 'market_code': str})
        if 'code' in previous_df.columns:
            previous_df['code'] = previous_df['code'].astype(str).str.replace("'", "")
    except (FileNotFoundError, pd.errors.EmptyDataError):
        logger.warning(f"å‰ä¸€äº¤æ˜“æ—¥æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨æˆ–ä¸ºç©º: {previous_file}")
        return f"ğŸ“Š ä»Šæ—¥è‚¡ç¥¨æ± æ›´æ–°å®Œæˆ\næ•°æ®é‡: {len(current_merged_df)}åªè‚¡ç¥¨\nâš ï¸ æ— å†å²æ•°æ®å¯¹æ¯”"
    except Exception as e:
        logger.warning(f"è¯»å–å‰ä¸€äº¤æ˜“æ—¥æ•°æ®å¤±è´¥: {e}")
        return f"ğŸ“Š ä»Šæ—¥è‚¡ç¥¨æ± æ›´æ–°å®Œæˆ\næ•°æ®é‡: {len(current_merged_df)}åªè‚¡ç¥¨\nâš ï¸ è¯»å–å†å²æ•°æ®å¤±è´¥"
    
    # æ¸…ç†å½“å‰æ•°æ®çš„codeåˆ—
    current_df_clean = current_merged_df.copy()
    if 'code' in current_df_clean.columns:
        current_df_clean['code'] = current_df_clean['code'].astype(str).str.replace("'", "")
    
    # åŸºäºè‚¡ç¥¨ä»£ç è®¡ç®—å˜åŠ¨ï¼ˆç¡®ä¿å”¯ä¸€æ€§ï¼‰
    current_codes = set(current_df_clean['code'].astype(str))
    previous_codes = set(previous_df['code'].astype(str))
    
    # è·å–æ–°å¢å’Œç§»é™¤çš„è‚¡ç¥¨ä»£ç 
    new_codes = current_codes - previous_codes
    removed_codes = previous_codes - current_codes
    
    # ä¿å­˜æ–°å¢å’Œå‡å°‘çš„è‚¡ç¥¨å®½è¡¨æ•°æ®åˆ°CSVæ–‡ä»¶
    save_dir = "data/csv"
    os.makedirs(save_dir, exist_ok=True)
    
    # ä¿å­˜æ–°å¢è‚¡ç¥¨å®½è¡¨æ•°æ®
    if len(new_codes) > 0:
        new_stocks_df = current_df_clean[current_df_clean['code'].astype(str).isin(new_codes)].copy()
        if 'é‡è¦åº¦' in new_stocks_df.columns:
            new_stocks_df = new_stocks_df.sort_values('é‡è¦åº¦', ascending=False)
        add_file_path = f"{save_dir}/merge_add_{date_str}.csv"
        new_stocks_df.to_csv(add_file_path, index=False, encoding='utf-8-sig')
        logger.info(f"æ–°å¢è‚¡ç¥¨å®½è¡¨æ•°æ®ä¿å­˜å®Œæˆ, è·¯å¾„: {add_file_path}, æ•°é‡: {len(new_stocks_df)}")
    
    # ä¿å­˜å‡å°‘è‚¡ç¥¨å®½è¡¨æ•°æ®
    if len(removed_codes) > 0:
        removed_stocks_df = previous_df[previous_df['code'].astype(str).isin(removed_codes)].copy()
        if 'é‡è¦åº¦' in removed_stocks_df.columns:
            removed_stocks_df = removed_stocks_df.sort_values('é‡è¦åº¦', ascending=False)
        remove_file_path = f"{save_dir}/merge_remove_{date_str}.csv"
        removed_stocks_df.to_csv(remove_file_path, index=False, encoding='utf-8-sig')
        logger.info(f"å‡å°‘è‚¡ç¥¨å®½è¡¨æ•°æ®ä¿å­˜å®Œæˆ, è·¯å¾„: {remove_file_path}, æ•°é‡: {len(removed_stocks_df)}")
    
    # æ„å»ºåŸºç¡€æ¶ˆæ¯
    date_fmt = f"{previous_date.strftime('%Y-%m-%d')}"
    message = [
        "ğŸ“Š æ¯æ—¥å¤ç›˜:è‚¡ç¥¨æ± ",
        f"ğŸ“… å¯¹æ¯”åŸºå‡†: {date_fmt}",
        f"ğŸ“ˆ ä»Šæ—¥: {len(current_codes)}åª | æ˜¨æ—¥: {len(previous_codes)}åª"
    ]
    
    if len(new_codes) == 0 and len(removed_codes) == 0:
        message.append("âœ¨ è‚¡ç¥¨æ± æ— å˜åŠ¨")
    else:
        # æ–°å¢è‚¡ç¥¨è¯¦æƒ…
        if len(new_codes) > 0:
            message.append(f"ğŸ†• æ–°å¢: {len(new_codes)}åª")
            
            # è·å–æ–°å¢è‚¡ç¥¨çš„è¯¦ç»†ä¿¡æ¯
            new_stocks_info = current_df_clean[current_df_clean['code'].astype(str).isin(new_codes)].copy()
            if 'é‡è¦åº¦' in new_stocks_info.columns:
                new_stocks_info = new_stocks_info.sort_values('é‡è¦åº¦', ascending=False)
            
            for _, stock in new_stocks_info.iterrows():
                code = stock['code']
                name = stock.get('è‚¡ç¥¨ç®€ç§°', 'æœªçŸ¥')
                interval_info = stock.get('åŒºé—´ä¿¡æ¯', 'æ— ')
                hotspot = stock.get('çƒ­ç‚¹', 'å…¶ä»–')
                message.append(f"  â€¢ {name}({code}) çƒ­ç‚¹:{hotspot} åŒºé—´:{interval_info}")
        
        # ç§»é™¤è‚¡ç¥¨ç»Ÿè®¡
        if len(removed_codes) > 0:
            message.append(f"ğŸ”» ç§»é™¤: {len(removed_codes)}åª")
            
            # è·å–ç§»é™¤è‚¡ç¥¨çš„è¯¦ç»†ä¿¡æ¯ï¼ˆä»å‰ä¸€å¤©çš„æ•°æ®ä¸­ï¼‰
            removed_stocks_info = previous_df[previous_df['code'].astype(str).isin(removed_codes)].copy()
            if 'é‡è¦åº¦' in removed_stocks_info.columns:
                removed_stocks_info = removed_stocks_info.sort_values('é‡è¦åº¦', ascending=False)
            
            for _, stock in removed_stocks_info.iterrows():
                code = stock['code']
                name = stock.get('è‚¡ç¥¨ç®€ç§°', 'æœªçŸ¥')
                interval_info = stock.get('åŒºé—´ä¿¡æ¯', 'æ— ')
                hotspot = stock.get('çƒ­ç‚¹', 'å…¶ä»–')
                message.append(f"  â€¢ {name}({code}) çƒ­ç‚¹:{hotspot} åŒºé—´:{interval_info}")
    
    msg = "\n".join(message)
    logger.info(msg)
    return msg

def load_all_data() -> Optional[Tuple]:
    """
    åŠ è½½æ‰€æœ‰å¸‚åœºæ•°æ®ï¼ŒåŒ…æ‹¬è‚¡ç¥¨æ± ã€è¡Œæƒ…æ•°æ®ã€æ¶¨åœæ•°æ®å’Œå¼‚åŠ¨æ•°æ®
    
    Returns:
        Optional[Tuple]: åŒ…å«æ‰€æœ‰æ•°æ®DataFrameçš„å…ƒç»„ï¼Œå¦‚æœåŠ è½½å¤±è´¥åˆ™è¿”å›None
            - core_stocks_data: æ ¸å¿ƒè‚¡ç¥¨æ± æ•°æ®
            - added_stocks_data: æ–°å¢è‚¡ç¥¨æ± æ•°æ®  
            - removed_stocks_data: å‡å°‘è‚¡ç¥¨æ± æ•°æ®
            - first_board_stocks_data: é¦–æ¿è‚¡ç¥¨æ± æ•°æ®
            - market_overview_data: åŒèŠ±é¡ºè¡Œæƒ…æ¦‚è§ˆæ•°æ®
            - zt_stocks_data: æ¶¨åœè‚¡ç¥¨æ•°æ®
            - jygs_data: å¼‚åŠ¨è‚¡ç¥¨æ•°æ®
    """
    try:
        logger.info("å¼€å§‹åŠ è½½å¸‚åœºæ•°æ®...")
        
        # åˆ›å»ºè‚¡ç¥¨æ± å®ä¾‹ï¼Œé¿å…é‡å¤åˆ›å»º
        stock_pool_manager = StockPool()
        
        logger.info("1.æ­£åœ¨è¯»å–æ ¸å¿ƒè‚¡ç¥¨æ± æ•°æ®...")
        core_stocks_data = stock_pool_manager.read_stock_pool_data(prefix='core_stocks')
        if core_stocks_data is None or core_stocks_data.empty:
            logger.error("æ ¸å¿ƒè‚¡ç¥¨æ± æ•°æ®ä¸ºç©º")
            raise Exception("æ ¸å¿ƒè‚¡ç¥¨æ± æ•°æ®ä¸ºç©º")
        logger.info(f"æ ¸å¿ƒè‚¡ç¥¨æ± æ•°æ®é‡: {len(core_stocks_data)}")

        logger.info("2.æ­£åœ¨è¯»å–é¦–æ¿è‚¡ç¥¨æ± æ•°æ®...")
        first_board_stocks_data = stock_pool_manager.read_stock_pool_data(prefix='first_stocks')
        if first_board_stocks_data is None or first_board_stocks_data.empty:
            logger.error("é¦–æ¿è‚¡ç¥¨æ± æ•°æ®ä¸ºç©º")
            raise Exception("é¦–æ¿è‚¡ç¥¨æ± æ•°æ®ä¸ºç©º")
        logger.info(f"é¦–æ¿è‚¡ç¥¨æ± æ•°æ®é‡: {len(first_board_stocks_data)}")
        
        logger.info("3. æ­£åœ¨è¯»å–å¸‚åœºè¡Œæƒ…æ•°æ®...")
        market_overview_data = WencaiUtils.read_market_overview_data()
        if market_overview_data is None or market_overview_data.empty:
            logger.error("å¸‚åœºè¡Œæƒ…æ•°æ®ä¸ºç©º")
            raise Exception("å¸‚åœºè¡Œæƒ…æ•°æ®ä¸ºç©º")
        logger.info(f"å¸‚åœºè¡Œæƒ…æ•°æ®é‡: {len(market_overview_data)}")
        
        logger.info("4.æ­£åœ¨è¯»å–æ›´æ–°åçš„æ¶¨åœæ•°æ®...")
        zt_stocks_data = WencaiUtils.read_latest_zt_stocks()
        if zt_stocks_data is None or zt_stocks_data.empty:
            logger.error("æ¶¨åœè‚¡ç¥¨æ•°æ®ä¸ºç©º")
            raise Exception("æ¶¨åœè‚¡ç¥¨æ•°æ®ä¸ºç©º")
        logger.info(f"æ¶¨åœè‚¡ç¥¨æ•°æ®é‡: {len(zt_stocks_data)}")
        zt_stocks_data.rename(columns={'äº¤æ˜“æ—¥æœŸ': 'æ¶¨åœæ—¥æœŸ'}, inplace=True)
        
        logger.info("5.æ­£åœ¨è¯»å–æ›´æ–°åçš„å¼‚åŠ¨æ•°æ®...")
        jygs_data = JygsUtils.read_stocks_data()
        if jygs_data is None or jygs_data.empty:
            logger.error("å¼‚åŠ¨è‚¡ç¥¨æ•°æ®ä¸ºç©º")
            raise Exception("å¼‚åŠ¨è‚¡ç¥¨æ•°æ®ä¸ºç©º")
        logger.info(f"å¼‚åŠ¨è‚¡ç¥¨æ•°æ®é‡: {len(jygs_data)}")
        jygs_data.rename(columns={'äº¤æ˜“æ—¥æœŸ': 'å¼‚åŠ¨æ—¥æœŸ'}, inplace=True)
        
        logger.info("æ‰€æœ‰å¿…è¦çš„æ•°æ®åŠ è½½å®Œæˆ")
        return (core_stocks_data, 
                first_board_stocks_data, market_overview_data, 
                zt_stocks_data, jygs_data)
                
    except Exception as e:
        logger.error(f"åŠ è½½å¸‚åœºæ•°æ®å¤±è´¥: {e}")
        raise

def clean_data(merged_data: pd.DataFrame) -> pd.DataFrame:
    """
    æ¸…æ´—å’Œæ ¼å¼åŒ–è´¢åŠ¡æ•°æ®ï¼Œå¹¶å¤„ç†æ‰€æœ‰ç±»å‹çš„NaNå€¼
    
    Args:
        merged_data: åˆå¹¶åçš„æ•°æ®DataFrame
        
    Returns:
        pd.DataFrame: æ¸…æ´—å’Œæ ¼å¼åŒ–åçš„æ•°æ®
    """
    try:
        # å®šä¹‰ä¸åŒç±»å‹åˆ—çš„NaNå¡«å……è§„åˆ™
        # æ•°å€¼å‹åˆ—ï¼šå¡«å……ä¸º0
        numeric_columns = ['å¸‚å€¼Z', 'æˆäº¤é¢', 'ç«ä»·é‡‘é¢', 'å¤§å•å‡€é¢', 'æ¶¨åœå°å•é¢', 
                          'çƒ­åº¦æ’å', 'ç«æ¢æ‰‹Z', 'è¿æ¿', 'å¼€æ¿æ¬¡æ•°', 'å°æˆé‡æ¯”', 'å°æµé‡æ¯”']
        
        # å­—ç¬¦ä¸²å‹åˆ—ï¼šå¡«å……ä¸ºç©ºå­—ç¬¦ä¸²æˆ–é»˜è®¤å€¼
        string_columns = ['çƒ­ç‚¹', 'çƒ­ç‚¹å¯¼ç«ç´¢', 'å¼‚åŠ¨åŸå› ', 'è§£æ']
        
        # å¤„ç†æ•°å€¼å‹åˆ—
        for column in numeric_columns:
            if column in merged_data.columns:
                # æ›¿æ¢æ— ç©·å€¼ä¸ºNaN
                merged_data[column] = merged_data[column].replace([np.inf, -np.inf], np.nan)
                # å¡«å……NaNä¸º0
                merged_data[column].fillna(0, inplace=True)
        
        # å¤„ç†å­—ç¬¦ä¸²å‹åˆ—
        for column in string_columns:
            if column in merged_data.columns:
                # å¡«å……NaNä¸ºç©ºå­—ç¬¦ä¸²
                merged_data[column].fillna('å…¶ä»–', inplace=True)
        
        # å°†é‡‘é¢å•ä½è½¬æ¢ä¸ºäº¿å…ƒï¼Œæ·»åŠ å¼‚å¸¸å¤„ç†
        conversion_config = {
            'å¸‚å€¼Z': (1e8, 0),      # è½¬æ¢ä¸ºäº¿å…ƒï¼Œä¿ç•™æ•´æ•°
            'æˆäº¤é¢': (1e8, 1),     # è½¬æ¢ä¸ºäº¿å…ƒï¼Œä¿ç•™1ä½å°æ•°
            'å¤§å•å‡€é¢': (1e8, 2),   # è½¬æ¢ä¸ºäº¿å…ƒï¼Œä¿ç•™2ä½å°æ•°
            'æ¶¨åœå°å•é¢': (1e8, 2), # è½¬æ¢ä¸ºäº¿å…ƒï¼Œä¿ç•™2ä½å°æ•°
            'ç«ä»·é‡‘é¢': (1e8, 2)    # è½¬æ¢ä¸ºäº¿å…ƒï¼Œä¿ç•™2ä½å°æ•°
        }
        
        for column, (divisor, decimal_places) in conversion_config.items():
            if column in merged_data.columns:
                try:
                    merged_data[column] = merged_data[column].apply(
                        lambda x: round(float(x) / divisor, decimal_places) if pd.notnull(x) else 0
                    )
                    # logger.info(f"å·²è½¬æ¢{column}åˆ—çš„å•ä½ä¸ºäº¿å…ƒ")
                except Exception as e:
                    logger.error(f"è½¬æ¢{column}åˆ—æ—¶å‘ç”Ÿé”™è¯¯: {e}")
                    # å¦‚æœè½¬æ¢å¤±è´¥ï¼Œä¿æŒåŸå€¼
                    continue
        
        return merged_data
        
    except Exception as e:
        logger.error(f"æ¸…æ´—å’Œæ ¼å¼åŒ–è´¢åŠ¡æ•°æ®æ—¶å‘ç”Ÿé”™è¯¯: {e}")
        return merged_data


class MarketData:
    """å¸‚åœºæ•°æ®å°è£…ç±»"""
    def __init__(self, market_overview: pd.DataFrame, zt_stocks: pd.DataFrame, jygs: pd.DataFrame):
        self.market_overview = market_overview
        self.zt_stocks = zt_stocks
        self.jygs = jygs

def merge_data(stock_pool_data: pd.DataFrame, 
               market_data: MarketData,
               output_prefix: str = 'core_stocks',
               date_str: str = None) -> Optional[pd.DataFrame]:
    """
    å°†è‚¡ç¥¨æ± æ•°æ®ä¸å¸‚åœºæ•°æ®è¿›è¡Œåˆå¹¶
    
    Args:
        stock_pool_data: è‚¡ç¥¨æ± æ•°æ®
        market_data: å¸‚åœºæ•°æ®å°è£…å¯¹è±¡
        output_prefix: è¾“å‡ºæ–‡ä»¶å‰ç¼€
        date_str: æ—¥æœŸå­—ç¬¦ä¸²ï¼Œé»˜è®¤ä¸ºNoneæ—¶ä½¿ç”¨å½“å‰äº¤æ˜“æ—¥
        
    Returns:
        Optional[pd.DataFrame]: åˆå¹¶åçš„æ•°æ®ï¼Œå¦‚æœåˆå¹¶å¤±è´¥åˆ™è¿”å›None
    """
    try:
        # è·å–æ—¥æœŸå­—ç¬¦ä¸²
        if date_str is None:
            date_str = trading_calendar.get_default_trade_date()
        
        logger.info(f"å¼€å§‹åˆå¹¶{output_prefix}æ•°æ®...")
        
        # éªŒè¯è¾“å…¥æ•°æ®    
        if not all([
            stock_pool_data is not None and not stock_pool_data.empty,
            market_data.market_overview is not None and not market_data.market_overview.empty,
            market_data.zt_stocks is not None and not market_data.zt_stocks.empty,
            market_data.jygs is not None and not market_data.jygs.empty
        ]):
            logger.error("è¾“å…¥æ•°æ®éªŒè¯å¤±è´¥ï¼Œæ— æ³•è¿›è¡Œåˆå¹¶")
            return None
        
        # é€‰æ‹©éœ€è¦çš„å¸‚åœºæ¦‚è§ˆåˆ—ï¼Œæ£€æŸ¥åˆ—æ˜¯å¦å­˜åœ¨
        market_columns = ['code', 'market_code', 'çƒ­åº¦æ’å', 'ç«æ¢æ‰‹Z', 'ç«ä»·é‡‘é¢', 
                         'ç«ä»·æ¶¨å¹…', 'æ¶¨è·Œå¹…', 'å®ä½“æ¶¨å¹…', 'æ¢æ‰‹Z', 'æˆäº¤é¢', 'å¤§å•å‡€é¢', 'ä¸Šå¸‚æ¿å—','å‡ å¤©å‡ æ¿']
        available_market_columns = [col for col in market_columns if col in market_data.market_overview.columns]
        
        # é€‰æ‹©éœ€è¦çš„æ¶¨åœæ•°æ®åˆ—
        zt_columns = ['code', 'market_code', 'æ¶¨åœæ—¥æœŸ', 'è¿æ¿', 'æ¶¨åœå°å•é¢', 
                           'æ¶¨åœåŸå› ç±»åˆ«', 'å°æˆé‡æ¯”', 'å°æµé‡æ¯”']
        available_zt_columns = [col for col in zt_columns if col in market_data.zt_stocks.columns]
        
        # é€‰æ‹©éœ€è¦çš„å¼‚åŠ¨æ•°æ®åˆ—
        jygs_columns = ['code', 'å¼‚åŠ¨æ—¥æœŸ', 'çƒ­ç‚¹', 'çƒ­ç‚¹å¯¼ç«ç´¢', 'å¼‚åŠ¨åŸå› ', 'è§£æ']
        available_jygs_columns = [col for col in jygs_columns if col in market_data.jygs.columns]
        
        # å¤„ç†ç©ºè‚¡ç¥¨æ± çš„æƒ…å†µ
        if stock_pool_data.empty:
            logger.info(f"{output_prefix}è‚¡ç¥¨æ± ä¸ºç©ºï¼Œåˆ›å»ºç©ºçš„åˆå¹¶ç»“æœ")
            # åˆ›å»ºç©ºçš„DataFrameï¼ŒåŒ…å«å¿…è¦çš„åˆ—ç»“æ„
            merged_data = pd.DataFrame(columns=['code'])
        else:
            # é€æ­¥åˆå¹¶æ•°æ®
            logger.info("æ­£åœ¨åˆå¹¶å¸‚åœºæ¦‚è§ˆæ•°æ®...")
            merged_data = pd.merge(stock_pool_data, 
                                  market_data.market_overview[available_market_columns], 
                                  on=['code', 'market_code'], 
                                  how='left')
            
            logger.info("æ­£åœ¨åˆå¹¶æ¶¨åœæ•°æ®...")
            merged_data = pd.merge(merged_data, 
                                  market_data.zt_stocks[available_zt_columns], 
                                  on=['code', 'market_code'], 
                                  how='left')
            logger.info("æ­£åœ¨åˆå¹¶å¼‚åŠ¨æ•°æ®...")
            merged_data = pd.merge(merged_data, 
                                  market_data.jygs[available_jygs_columns], 
                                  on=['code'], 
                                  how='left')
        
        # æ ¼å¼åŒ–ä»£ç å’ŒåŒºé—´ä¿¡æ¯ï¼ˆæ·»åŠ å•å¼•å·å‰ç¼€ï¼‰
        if 'code' in merged_data.columns and not merged_data.empty:
            merged_data['code'] = merged_data['code'].apply(lambda x: f"'{x}")
        
        if 'åŒºé—´ä¿¡æ¯' in merged_data.columns and not merged_data.empty:
            merged_data['åŒºé—´ä¿¡æ¯'] = merged_data['åŒºé—´ä¿¡æ¯'].apply(lambda x: f"'{x}")
        
        # åˆ é™¤ä¸éœ€è¦çš„åˆ—
        if 'market_code' in merged_data.columns:
            merged_data.drop(columns=['market_code'], inplace=True)
        
        # æ¸…æ´—å’Œæ ¼å¼åŒ–è´¢åŠ¡æ•°æ®
        merged_data = clean_data(merged_data)
        
        # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
        os.makedirs(OUTPUT_DIR, exist_ok=True)
        
        # ä¿å­˜åˆå¹¶åçš„æ•°æ®ï¼ˆåŠ ä¸Šæ—¥æœŸï¼‰
        output_path = f'{OUTPUT_DIR}/{OUTPUT_FILE_PREFIX}{output_prefix}_{date_str}.csv'
        merged_data.to_csv(output_path, index=False, encoding='utf-8-sig')
        logger.info(f"{output_prefix}æ•°æ®åˆå¹¶å®Œæˆï¼Œå·²ä¿å­˜åˆ°: {output_path}")
        
        return merged_data
        
    except Exception as e:
        logger.error(f"åˆå¹¶{output_prefix}æ•°æ®å¤±è´¥: {e}")
        return None



def identify_emerging_hotspots(date_str: str = None, lookback_days: int = 10) -> list:
    """
    è¯†åˆ«ä»Šæ—¥æ–°å‡ºç°çš„çƒ­ç‚¹
    
    Args:
        lookback_days: å›çœ‹çš„äº¤æ˜“æ—¥å¤©æ•°ï¼Œé»˜è®¤10å¤©
        
    Returns:
        list: ä»Šæ—¥æ–°å‡ºç°çš„çƒ­ç‚¹åˆ—è¡¨
    """
    def is_hotspot_similar(hotspot_a: str, hotspot_b: str) -> bool:
        """
        åˆ¤æ–­ä¸¤ä¸ªçƒ­ç‚¹æ˜¯å¦ç›¸ä¼¼(é€šè¿‡å­—ç¬¦ä¸²åŒ…å«å…³ç³»)
        
        Args:
            hotspot_a: çƒ­ç‚¹A
            hotspot_b: çƒ­ç‚¹B
            
        Returns:
            bool: å¦‚æœå­˜åœ¨åŒ…å«å…³ç³»åˆ™è¿”å›True
        """
        return (hotspot_a in hotspot_b) or (hotspot_b in hotspot_a)
    
    try:
        # è¯»å–æ¿å—å†å²æ•°æ®
        bk_historical_data = JygsUtils.read_bk_data()
        if bk_historical_data.empty:
            logger.error("æ¿å—å†å²æ•°æ®ä¸ºç©º")
            raise Exception("æ¿å—å†å²æ•°æ®ä¸ºç©º")
            
        trading_dates = bk_historical_data['äº¤æ˜“æ—¥æœŸ'].unique()
        if len(trading_dates) == 0 or date_str != trading_dates[0]:
            logger.error("æ²¡æœ‰æ‰¾åˆ°äº¤æ˜“æ—¥æœŸçš„æ¿å—æ•°æ®")
            raise Exception("æ²¡æœ‰æ‰¾åˆ°äº¤æ˜“æ—¥æœŸçš„æ¿å—æ•°æ®")
        
        # è·å–è¿‡å»Nä¸ªäº¤æ˜“æ—¥çš„æ—¥æœŸèŒƒå›´
        past_trading_dates = trading_dates[1:lookback_days+1] if len(trading_dates) > lookback_days else trading_dates[1:]
        
        # æå–è¿‡å»Nå¤©çš„æ‰€æœ‰çƒ­ç‚¹
        past_hotspots = bk_historical_data[bk_historical_data['äº¤æ˜“æ—¥æœŸ'].isin(past_trading_dates)]['çƒ­ç‚¹'].unique()
        # è·å–ä»Šæ—¥çƒ­ç‚¹æ•°æ®
        latest_trading_date = trading_dates[0]
        today_data = bk_historical_data[bk_historical_data['äº¤æ˜“æ—¥æœŸ'] == latest_trading_date]
        today_hotspots = today_data['çƒ­ç‚¹'].unique()
        # ç­›é€‰æ–°å…´çƒ­ç‚¹
        emerging_hotspots = []
        for today_hotspot in today_hotspots:
            is_existing = False
            for past_hotspot in past_hotspots:
                if is_hotspot_similar(today_hotspot, past_hotspot):
                    is_existing = True
                    break
            
            if not is_existing and today_hotspot != 'å…¶ä»–':
                emerging_hotspots.append(today_hotspot)
        
        logger.info(f"è¯†åˆ«åˆ° {len(emerging_hotspots)} ä¸ªæ–°å…´çƒ­ç‚¹: {emerging_hotspots}")
        return emerging_hotspots
        
    except Exception as e:
        logger.error(f"è¯†åˆ«æ–°å…´çƒ­ç‚¹æ—¶å‘ç”Ÿé”™è¯¯: {e}")
        raise



def generate_report(merged_first_board_data: pd.DataFrame, 
                                     emerging_hotspots: list,
                                     date_str: str = None) -> str:
    """
    ç”Ÿæˆæ–°å…´çƒ­ç‚¹æŠ¥å‘Šï¼Œä¿å­˜å®½è¡¨æ•°æ®å¹¶ç”Ÿæˆæ ¼å¼åŒ–æ¶ˆæ¯
    
    Args:
        merged_first_board_data: åˆå¹¶åçš„é¦–æ¿è‚¡ç¥¨å®½è¡¨æ•°æ®
        emerging_hotspots: æ–°å…´çƒ­ç‚¹åˆ—è¡¨
        date_str: æ—¥æœŸå­—ç¬¦ä¸²ï¼Œé»˜è®¤ä¸ºNoneæ—¶ä½¿ç”¨å½“å‰äº¤æ˜“æ—¥
        
    Returns:
        str: æ ¼å¼åŒ–çš„çƒ­ç‚¹æŠ¥å‘Šæ¶ˆæ¯
    """
    if date_str is None:
        date_str = trading_calendar.get_default_trade_date()
    
    if not emerging_hotspots or len(emerging_hotspots) == 0:
        return "ğŸ“Š ä»Šæ—¥æ–°å…´çƒ­ç‚¹åˆ†æ\nâœ¨ æš‚æ— æ–°å‡ºç°çš„çƒ­ç‚¹"
    
    try:
        # ç­›é€‰å‡ºæ–°å…´çƒ­ç‚¹ç›¸å…³çš„è‚¡ç¥¨
        emerging_hotspots_df = merged_first_board_data[
            merged_first_board_data['çƒ­ç‚¹'].isin(emerging_hotspots)
        ].copy()
        
        if emerging_hotspots_df.empty:
            return f"ğŸ“Š ä»Šæ—¥æ–°å…´çƒ­ç‚¹åˆ†æ\nğŸ” å‘ç°{len(emerging_hotspots)}ä¸ªæ–°çƒ­ç‚¹ï¼Œä½†æ— ç›¸å…³é¦–æ¿è‚¡ç¥¨æ•°æ®"
        
        # ä¿å­˜æ–°å…´çƒ­ç‚¹å®½è¡¨æ•°æ®åˆ°csvæ–‡ä»¶
        save_dir = "data/csv"
        os.makedirs(save_dir, exist_ok=True)
        
        emerging_hotspots_file = f"{save_dir}/merge_emerging_hotspots_{date_str}.csv"
        emerging_hotspots_df.to_csv(emerging_hotspots_file, index=False, encoding='utf-8-sig')
        logger.info(f"æ–°å…´çƒ­ç‚¹å®½è¡¨æ•°æ®ä¿å­˜å®Œæˆ, è·¯å¾„: {emerging_hotspots_file}, æ•°é‡: {len(emerging_hotspots_df)}")
        
        # æ„å»ºæ ¼å¼åŒ–æ¶ˆæ¯
        message = [
            "ğŸ“Š ä»Šæ—¥æ–°å…´çƒ­ç‚¹åˆ†æ",
            f"ğŸ”¥ å‘ç°{len(emerging_hotspots)}ä¸ªæ–°çƒ­ç‚¹ï¼Œæ¶‰åŠ{len(emerging_hotspots_df)}åªé¦–æ¿è‚¡ç¥¨",
            ""
        ]
        
        # æŒ‰çƒ­ç‚¹åˆ†ç»„å±•ç¤ºè‚¡ç¥¨ä¿¡æ¯
        for hotspot in emerging_hotspots:
            hotspot_stocks = emerging_hotspots_df[emerging_hotspots_df['çƒ­ç‚¹'] == hotspot]
            if not hotspot_stocks.empty:
                message.append(f"ğŸ¯ çƒ­ç‚¹: {hotspot}")
                
                # æŒ‰é‡è¦åº¦æ’åºï¼ˆå¦‚æœæœ‰çš„è¯ï¼‰
                if 'é‡è¦åº¦' in hotspot_stocks.columns:
                    hotspot_stocks = hotspot_stocks.sort_values('é‡è¦åº¦', ascending=False)
                
                for _, stock in hotspot_stocks.iterrows():
                    # æ¸…ç†codeåˆ—çš„å•å¼•å·å‰ç¼€
                    code = str(stock['code']).replace("'", "")
                    name = stock.get('è‚¡ç¥¨ç®€ç§°', 'æœªçŸ¥')
                    jygs_reason = stock.get('å¼‚åŠ¨åŸå› ', 'å…¶ä»–')
                    hotspot_trigger = stock.get('çƒ­ç‚¹å¯¼ç«ç´¢', 'æ— ')
                    
                    # æ ¼å¼åŒ–è‚¡ç¥¨ä¿¡æ¯
                    stock_info = f"  â€¢ {name}({code})"
                    if jygs_reason and jygs_reason != 'å…¶ä»–':
                        stock_info += f" | å¼‚åŠ¨åŸå› : {jygs_reason}"
                    if hotspot_trigger and hotspot_trigger != 'æ— ':
                        stock_info += f" | å¯¼ç«ç´¢: {hotspot_trigger}"
                    
                    message.append(stock_info)
                
                message.append("")  # æ·»åŠ ç©ºè¡Œåˆ†éš”ä¸åŒçƒ­ç‚¹
        
        msg = "\n".join(message)
        logger.info(msg)
        return msg
        
    except Exception as e:
        logger.error(f"ç”Ÿæˆæ–°å…´çƒ­ç‚¹æŠ¥å‘Šæ—¶å‘ç”Ÿé”™è¯¯: {e}")
        return f"ğŸ“Š ä»Šæ—¥æ–°å…´çƒ­ç‚¹åˆ†æ\nâŒ ç”ŸæˆæŠ¥å‘Šæ—¶å‘ç”Ÿé”™è¯¯: {e}"


def merge():
    """
    åˆå¹¶è‚¡ç¥¨æ± æ•°æ®ã€å¸‚åœºæ•°æ®ã€æ¶¨åœæ•°æ®å’Œå¼‚åŠ¨æ•°æ®
    """
    try:
        logger.info("å¼€å§‹æ‰§è¡Œåˆå¹¶æ•°æ®ä¸»æµç¨‹")
        
        # è·å–å½“å‰äº¤æ˜“æ—¥æœŸ
        current_date = trading_calendar.get_default_trade_date()
        logger.info(f"å½“å‰äº¤æ˜“æ—¥æœŸ: {current_date}")
        
        logger.info("------------ æ­¥éª¤1: åŠ è½½è‚¡ç¥¨æ± æ•°æ®------------")
        loaded_data = load_all_data()
        (core_stocks_data, 
         first_board_stocks_data, 
         market_overview_data, 
         zt_stocks_data, 
         jygs_data) = loaded_data
        # å°è£…å¸‚åœºæ•°æ®
        market_data = MarketData(market_overview_data, zt_stocks_data, jygs_data)

        logger.info("------------ æ­¥éª¤2: åˆå¹¶æ ¸å¿ƒè‚¡ç¥¨æ± æ•°æ®------------")
        merged_core_data = merge_data(core_stocks_data, market_data, 'core_stocks', current_date)
        
        if merged_core_data is None:
            raise Exception("æ ¸å¿ƒè‚¡ç¥¨æ± æ•°æ®åˆå¹¶å¤±è´¥")
        logger.info("æ ¸å¿ƒè‚¡ç¥¨æ± æ•°æ®åˆå¹¶å®Œæˆ")

        logger.info("------------ æ­¥éª¤3: æ ¸å¿ƒè‚¡ç¥¨æ± ä¸ä¸Šä¸€ä¸ªäº¤æ˜“æ—¥å¯¹æ¯”åˆ†æ------------")
        comparison_msg = compare_previous(merged_core_data, current_date)
        dingding_robot.send_message(comparison_msg, 'robot3')
        logger.info("æ ¸å¿ƒè‚¡ç¥¨æ± å¯¹æ¯”åˆ†æå®Œæˆ")

        logger.info("------------ æ­¥éª¤4: åˆå¹¶é¦–æ¿è‚¡ç¥¨æ± æ•°æ®------------")

        merged_first_board_data = merge_data(first_board_stocks_data, market_data, 'first_stocks', current_date)
        
        if merged_first_board_data is None:
            raise Exception("é¦–æ¿è‚¡ç¥¨æ± æ•°æ®åˆå¹¶å¤±è´¥")
        logger.info("é¦–æ¿è‚¡ç¥¨æ± æ•°æ®åˆå¹¶å®Œæˆ")

        logger.info("------------ æ­¥éª¤5: è¯†åˆ«ä»Šæ—¥æ–°å‡ºç°çš„çƒ­ç‚¹------------")
        emerging_hotspots = identify_emerging_hotspots(date_str=current_date)
        
        # ç”Ÿæˆæ–°å…´çƒ­ç‚¹æŠ¥å‘Šå¹¶å‘é€æ¶ˆæ¯
        hotspots_report_msg = generate_report(
            merged_first_board_data, 
            emerging_hotspots, 
            current_date
        )
        dingding_robot.send_message(hotspots_report_msg, 'robot3')
        logger.info("æ–°å…´çƒ­ç‚¹åˆ†æå®Œæˆ")

        logger.info("åˆå¹¶æ•°æ®ä¸»æµç¨‹æ‰§è¡Œå®Œæˆ")
    
    except Exception as e:
        logger.error(f"ç¨‹åºæ‰§è¡Œå¤±è´¥: {e}")
        dingding_robot.send_message(f"æ¯æ—¥æ•°æ®å¤„ç†å¤±è´¥: {e}", 'robot3')
        raise


if __name__ == "__main__":
    merge()
