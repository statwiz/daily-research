import pywencai
import re
import pandas as pd
from typing import Optional
import numpy as np
import time
import os
from datetime import datetime, timedelta
from log_setup import get_logger
from utils import send_dingding_msg, trading_calendar
import akshare as ak
import configparser
import matplotlib.pyplot as plt
import matplotlib.patches as patches
from matplotlib import font_manager

import warnings
warnings.filterwarnings("ignore")

# é…ç½®ç±» - ç»Ÿä¸€ç®¡ç†æ‰€æœ‰å‚æ•°
class StockPoolConfig:
    """è‚¡ç¥¨æ± é…ç½®ç±»"""
    # é‡è¯•é…ç½®
    MAX_RETRY_COUNT = 3
    RETRY_BASE_DELAY = 2  # åŸºç¡€ç­‰å¾…æ—¶é—´ï¼Œå®é™…ç­‰å¾…æ—¶é—´ä¸º (attempt + 1) * BASE_DELAY
    
    # åŒºé—´é…ç½® [(å¤©æ•°, æ’å)]
    INTERVAL_CONFIGS = [
        (2, 10),   # æœ€è¿‘2æ—¥å‰10å
        (3, 10),   # æœ€è¿‘3æ—¥å‰10å  
        # (5, 10),   # æœ€è¿‘5æ—¥å‰10å
        # (10, 5),   # æœ€è¿‘10æ—¥å‰5å
        # (15, 3),   # æœ€è¿‘15æ—¥å‰3å
        # (20, 2)    # æœ€è¿‘20æ—¥å‰2å
    ]
    
    # é‡è¦åº¦è®¡ç®—å‚æ•°
    IMPORTANCE_ALPHA = 0.999  # æ’åæƒé‡ï¼Œè¶Šå¤§è¶Šå¼ºè°ƒæ’åå·®è·
    IMPORTANCE_BETA = 0.001   # åŒºé—´é•¿åº¦æƒé‡ï¼Œè¶Šå¤§è¶Šå¼ºè°ƒçŸ­åŒºé—´
    
    # æ•°æ®ä¿å­˜é…ç½®
    DATA_SAVE_DIR = 'data'
    CSV_SUBDIR = 'csv'
    IMAGES_SUBDIR = 'images'
    DATA_FILE_PREFIX = 'stock_pool'
    
    # å›¾ç‰‡ç”Ÿæˆé…ç½®
    IMAGE_WIDTH = 12  # å›¾ç‰‡å®½åº¦(è‹±å¯¸)
    IMAGE_HEIGHT = 8  # å›¾ç‰‡é«˜åº¦(è‹±å¯¸) 
    IMAGE_DPI = 150   # å›¾ç‰‡åˆ†è¾¨ç‡
    FONT_SIZE = 14    # å­—ä½“å¤§å°
    
    @classmethod
    def get_csv_save_path(cls, date_str: str = None) -> str:
        """ç”ŸæˆCSVæ–‡ä»¶ä¿å­˜è·¯å¾„"""
        if date_str is None:
            date_str = datetime.now().strftime('%Y%m%d')
        return f"{cls.DATA_SAVE_DIR}/{cls.CSV_SUBDIR}/{cls.DATA_FILE_PREFIX}_{date_str}.csv"
    
    @classmethod
    def get_image_save_path(cls, date_str: str = None) -> str:
        """ç”Ÿæˆå›¾ç‰‡æ–‡ä»¶ä¿å­˜è·¯å¾„"""
        if date_str is None:
            date_str = datetime.now().strftime('%Y%m%d')
        return f"{cls.DATA_SAVE_DIR}/{cls.IMAGES_SUBDIR}/{cls.DATA_FILE_PREFIX}_{date_str}.png"
    
    @classmethod 
    def get_data_save_path(cls, date_str: str = None) -> str:
        """å…¼å®¹æ€§æ–¹æ³•ï¼Œè¿”å›CSVæ–‡ä»¶ä¿å­˜è·¯å¾„"""
        return cls.get_csv_save_path(date_str)
    
    # æŸ¥è¯¢ç­‰å¾…æ—¶é—´
    QUERY_SLEEP_SECONDS = 3
    MAIN_RETRY_SLEEP_SECONDS = 10

# æ—¥å¿—é…ç½®
logger = get_logger("stock_pool", "logs", "stock_pool.log")




def compare_with_previous_trading_day(current_df: pd.DataFrame, date_str: str = None) -> str:
    """ä¸å‰ä¸€ä¸ªäº¤æ˜“æ—¥çš„æ•°æ®è¿›è¡Œå¯¹æ¯”ï¼Œè¿”å›å¯¹æ¯”æ¶ˆæ¯"""
    if date_str is None:
        date_str = datetime.now().strftime('%Y%m%d')
    
    # è·å–å‰ä¸€äº¤æ˜“æ—¥æ•°æ®
    previous_date = trading_calendar.get_previous_trading_day(date_str)
    if not previous_date:
        return f"ğŸ“Š ä»Šæ—¥è‚¡ç¥¨æ± æ›´æ–°å®Œæˆ\næ•°æ®é‡: {len(current_df)}åªè‚¡ç¥¨\nâš ï¸ æœªæ‰¾åˆ°å‰ä¸€äº¤æ˜“æ—¥æ•°æ®"
    
    previous_file = StockPoolConfig.get_data_save_path(previous_date.strftime('%Y%m%d'))
    if not os.path.exists(previous_file):
        return f"ğŸ“Š ä»Šæ—¥è‚¡ç¥¨æ± æ›´æ–°å®Œæˆ\næ•°æ®é‡: {len(current_df)}åªè‚¡ç¥¨\nâš ï¸ æœªæ‰¾åˆ°å‰ä¸€äº¤æ˜“æ—¥æ•°æ®æ–‡ä»¶"
    
    try:
        previous_df = pd.read_csv(previous_file)
    except:
        return f"ğŸ“Š ä»Šæ—¥è‚¡ç¥¨æ± æ›´æ–°å®Œæˆ\næ•°æ®é‡: {len(current_df)}åªè‚¡ç¥¨\nâš ï¸ è¯»å–å‰ä¸€äº¤æ˜“æ—¥æ•°æ®å¤±è´¥"
    
    # åŸºäºè‚¡ç¥¨ä»£ç è®¡ç®—å˜åŠ¨ï¼ˆç¡®ä¿å”¯ä¸€æ€§ï¼‰
    current_codes = set(current_df['code'].astype(str))
    previous_codes = set(previous_df['code'].astype(str))
    
    # è·å–æ–°å¢å’Œç§»é™¤çš„è‚¡ç¥¨ä»£ç 
    new_codes = current_codes - previous_codes
    removed_codes = previous_codes - current_codes
    
    # æ„å»ºåŸºç¡€æ¶ˆæ¯
    date_fmt = f"{previous_date.strftime('%Y-%m-%d')}"
    message = [
        "ğŸ“Š æ¯æ—¥å¤ç›˜:è‚¡ç¥¨æ± ",
        f"ğŸ“… å¯¹æ¯”åŸºå‡†: {date_fmt}",
        f"ğŸ“ˆ ä»Šæ—¥: {len(current_codes)}åª | æ˜¨æ—¥: {len(previous_codes)}åª"
    ]
    
    if len(new_codes) == 0 and len(removed_codes) == 0:
        message.append("âœ¨ è‚¡ç¥¨æ± æ— å˜åŠ¨")
    else:
        # æ–°å¢è‚¡ç¥¨è¯¦æƒ…
        if len(new_codes) > 0:
            message.append(f"ğŸ†• æ–°å¢: {len(new_codes)}åª")
            
            # è·å–æ–°å¢è‚¡ç¥¨çš„è¯¦ç»†ä¿¡æ¯
            new_stocks_info = current_df[current_df['code'].astype(str).isin(new_codes)].copy()
            new_stocks_info = new_stocks_info.sort_values('é‡è¦åº¦', ascending=False)
            
            for _, stock in new_stocks_info.iterrows():
                code = stock['code']
                name = stock['è‚¡ç¥¨ç®€ç§°']
                interval_info = stock.get('åŒºé—´ä¿¡æ¯', 'æ— ')
                message.append(f"  â€¢ {name}({code}) åŒºé—´:{interval_info}")
        
        # ç§»é™¤è‚¡ç¥¨ç»Ÿè®¡
        if len(removed_codes) > 0:
            message.append(f"ğŸ”» ç§»é™¤: {len(removed_codes)}åª")
            
            # è·å–ç§»é™¤è‚¡ç¥¨çš„è¯¦ç»†ä¿¡æ¯ï¼ˆä»å‰ä¸€å¤©çš„æ•°æ®ä¸­ï¼‰
            removed_stocks_info = previous_df[previous_df['code'].astype(str).isin(removed_codes)].copy()
            removed_stocks_info = removed_stocks_info.sort_values('é‡è¦åº¦', ascending=False)
            
            for _, stock in removed_stocks_info.iterrows():
                code = stock['code']
                name = stock['è‚¡ç¥¨ç®€ç§°']
                interval_info = stock.get('åŒºé—´ä¿¡æ¯', 'æ— ')
                message.append(f"  â€¢ {name}({code}) åŒºé—´:{interval_info}")
    
    msg = "\n".join(message)
    logger.info(msg)
    return msg


def extract_trade_date(df: pd.DataFrame) -> Optional[str]:
    """
    ä» DataFrame çš„åˆ—åä¸­æå–ç¬¬ä¸€ä¸ªå½¢å¦‚ [YYYYMMDD] çš„æ—¥æœŸå­—ç¬¦ä¸²ã€‚
    å¦‚æœæœªæ‰¾åˆ°åˆ™è¿”å› Noneã€‚
    """
    for col in df.columns:
        single_date_match = re.search(r'\[(\d{8})\]', col)
        if single_date_match:
            return single_date_match.group(1)
        date_range_match = re.search(r'\[(\d{8}-\d{8})\]', col)
        if date_range_match:
            return date_range_match.group(1).split('-')[1]
    return None


def remove_date_suffix(df: pd.DataFrame) -> pd.DataFrame:
    """
    åˆ é™¤ DataFrame åˆ—åä¸­å½¢å¦‚ [YYYYMMDD]å’Œ[YYYYMMDD-YYYYMMDD] çš„æ—¥æœŸåç¼€ã€‚
    è¿”å›ä¿®æ”¹åçš„ DataFrameï¼ˆä¸ä¼šä¿®æ”¹åŸ dfï¼‰ã€‚
    """
    column_mapping = {}
    for col in df.columns:
        if re.search(r'\[\d{8}\]', col):
            new_name = re.sub(r'\[\d{8}\]', '', col)
            column_mapping[col] = new_name
        elif re.search(r'\[\d{8}-\d{8}\]', col):
            new_name = re.sub(r'\[\d{8}-\d{8}\]', '', col)
            column_mapping[col] = new_name
    return df.rename(columns=column_mapping)

def calc_importance(df: pd.DataFrame, 
                   alpha: float = None, 
                   beta: float = None) -> pd.DataFrame:
    """
    åŸºäºåŸå§‹æ•°æ®èšåˆå¹¶è®¡ç®—è‚¡ç¥¨é‡è¦åº¦
    
    Args:
        df: åŒ…å«è‚¡ç¥¨åŒºé—´æ•°æ®çš„DataFrame
        alpha: æ’åæƒé‡å‚æ•°ï¼Œè¶Šå¤§è¶Šå¼ºè°ƒæ’åå·®è·
        beta: åŒºé—´é•¿åº¦æƒé‡å‚æ•°ï¼Œè¶Šå¤§è¶Šå¼ºè°ƒçŸ­åŒºé—´
    
    Returns:
        æŒ‰é‡è¦åº¦æ’åºçš„DataFrameï¼ŒåŒ…å«åŒºé—´ä¿¡æ¯å’Œé‡è¦åº¦å¾—åˆ†
        
    è®¡ç®—é€»è¾‘:
        1. åˆ†ç»„å­—æ®µ: [äº¤æ˜“æ—¥æœŸ, è‚¡ç¥¨ç®€ç§°, market_code, code]
        2. åŒºé—´ä¿¡æ¯: 'åŒºé—´é•¿åº¦-åŒºé—´æ’å' ç”¨ | è¿æ¥
        3. é‡è¦åº¦: 100 * sum(exp(-alpha * æ’å) * exp(-beta * åŒºé—´é•¿åº¦))
        4. æ’åº: æŒ‰é‡è¦åº¦é™åº
    """
    # ä½¿ç”¨é…ç½®é»˜è®¤å€¼
    if alpha is None:
        alpha = StockPoolConfig.IMPORTANCE_ALPHA
    if beta is None:
        beta = StockPoolConfig.IMPORTANCE_BETA
    
    logger.info(f"å¼€å§‹è®¡ç®—è‚¡ç¥¨é‡è¦åº¦, æ•°æ®é‡: {len(df)}, alpha: {alpha}, beta: {beta}")
    start_time = time.time()
    
    def aggregate_importance(sub_df):
        """èšåˆå‡½æ•°ï¼šè®¡ç®—å•åªè‚¡ç¥¨çš„é‡è¦åº¦"""
        interval_info = "|".join(
            f"{length}-{rank}" for length, rank in zip(sub_df["åŒºé—´é•¿åº¦"], sub_df["åŒºé—´æ’å"])
        )
        
        # é‡è¦åº¦è®¡ç®—ï¼šä½¿ç”¨æŒ‡æ•°è¡°å‡
        importance_score = float(100 * (
            np.exp(-alpha * sub_df["åŒºé—´æ’å"]) * 
            np.exp(-beta * sub_df["åŒºé—´é•¿åº¦"])
        ).sum())
        
        return pd.Series({
            "åŒºé—´ä¿¡æ¯": interval_info,
            "é‡è¦åº¦": importance_score
        })

    # æŒ‰è‚¡ç¥¨åˆ†ç»„å¹¶è®¡ç®—é‡è¦åº¦
    grouped_df = (
        df.groupby(["äº¤æ˜“æ—¥æœŸ", "è‚¡ç¥¨ç®€ç§°", "market_code", "code"], as_index=False)
        .apply(aggregate_importance)
    )

    # æŒ‰é‡è¦åº¦é™åºæ’åº
    grouped_df.sort_values(by="é‡è¦åº¦", ascending=False, inplace=True)
    grouped_df.reset_index(drop=True, inplace=True)
    
    processing_time = time.time() - start_time
    logger.info(f"é‡è¦åº¦è®¡ç®—å®Œæˆ, è€—æ—¶: {processing_time:.2f}ç§’, ç»“æœæ•°é‡: {len(grouped_df)}")
    
    return grouped_df[["äº¤æ˜“æ—¥æœŸ", "è‚¡ç¥¨ç®€ç§°", "market_code", "code", "åŒºé—´ä¿¡æ¯", "é‡è¦åº¦"]]

def get_top_stocks(days: int = 5, rank: int = 5, use_filters: bool = False) -> pd.DataFrame:
    """
    è·å–æŒ‡å®šå¤©æ•°å†…æ¶¨å¹…æ’åå‰Nçš„è‚¡ç¥¨æ•°æ®
    
    Args:
        days: ç»Ÿè®¡å¤©æ•°
        rank: å–å‰Nå
        use_filters: æ˜¯å¦ä½¿ç”¨ç­›é€‰æ¡ä»¶ï¼ˆéSTã€éé€€å¸‚ã€ä¸Šå¸‚æ—¶é—´>30å¤©ã€æµé€šå¸‚å€¼>100äº¿ï¼‰
    
    Returns:
        å¤„ç†åçš„è‚¡ç¥¨æ•°æ®DataFrame
    """
    logger.info(f"å¼€å§‹è·å–{days}æ—¥å†…å‰{rank}åè‚¡ç¥¨æ•°æ®, ä½¿ç”¨ç­›é€‰: {use_filters}")
    start_time = time.time()
    
    # æ„å»ºæŸ¥è¯¢è¯­å¥
    if use_filters:
        query_text = (f"éST,è‚¡ç¥¨ç®€ç§°ä¸åŒ…å«é€€,ä¸Šå¸‚å¤©æ•°å¤§äº30,æµé€šå¸‚å€¼å¤§äº100äº¿,"
                     f"æœ€è¿‘{days}ä¸ªäº¤æ˜“æ—¥çš„åŒºé—´æ¶¨è·Œå¹…ä»å¤§åˆ°å°æ’åºå‰{rank}")
    else:
        query_text = f"æœ€è¿‘{days}ä¸ªäº¤æ˜“æ—¥çš„åŒºé—´æ¶¨è·Œå¹…ä»å¤§åˆ°å°æ’åºå‰{rank}"
    
    logger.info(f"æŸ¥è¯¢è¯­å¥: {query_text}")
    
    try:
        # è°ƒç”¨é—®è´¢APIè·å–æ•°æ®
        raw_df = pywencai.get(query=query_text, query_type='stock')
        logger.info(f"åŸå§‹æ•°æ®è·å–æˆåŠŸ, æ•°æ®é‡: {len(raw_df)}")
        
        # æ•°æ®å¤„ç†
        df = raw_df.copy()
        df['äº¤æ˜“æ—¥æœŸ'] = extract_trade_date(df)
        df = remove_date_suffix(df)
        
        # é‡å‘½ååˆ—
        column_mapping = {
            'åŒºé—´æ¶¨è·Œå¹…:å‰å¤æƒ': 'åŒºé—´æ¶¨å¹…', 
            'åŒºé—´æ¶¨è·Œå¹…:å‰å¤æƒæ’å': 'åŒºé—´æ’å'
        }
        df.rename(columns=column_mapping, inplace=True)
        
        # æ·»åŠ åŒºé—´é•¿åº¦
        df['åŒºé—´é•¿åº¦'] = days
        
        # æ•°æ®ç±»å‹è½¬æ¢
        # å¤„ç†æ’ååˆ—ï¼ˆæ ¼å¼ï¼š1/4532 -> 1ï¼‰
        df['åŒºé—´æ’å'] = (df['åŒºé—´æ’å']
                        .astype(str)
                        .str.split('/')
                        .str[0]
                        .astype(int))
        
        # å¤„ç†æ•°å€¼åˆ—
        if 'åŒºé—´æ¶¨å¹…' in df.columns:
            df['åŒºé—´æ¶¨å¹…'] = df['åŒºé—´æ¶¨å¹…'].astype(float).round(2)
        
        # å¤„ç†å­—ç¬¦ä¸²åˆ—
        for col in ['market_code', 'code']:
            if col in df.columns:
                df[col] = df[col].astype(str)
        
        # é€‰æ‹©éœ€è¦çš„åˆ—
        required_columns = ['äº¤æ˜“æ—¥æœŸ', 'è‚¡ç¥¨ç®€ç§°', 'åŒºé—´é•¿åº¦', 'åŒºé—´æ¶¨å¹…', 'åŒºé—´æ’å', 'market_code', 'code']
        result = df[required_columns]        
        return result
        
    except Exception as e:
        logger.error(f"è·å–è‚¡ç¥¨æ•°æ®å¤±è´¥: {e}")
        raise
    
def execute_with_retry(func, *args, **kwargs):
    """
    å¸¦é‡è¯•æœºåˆ¶çš„å‡½æ•°åŒ…è£…å™¨
    
    Args:
        func: è¦æ‰§è¡Œçš„å‡½æ•°
        *args, **kwargs: å‡½æ•°å‚æ•°
    
    Returns:
        å‡½æ•°æ‰§è¡Œç»“æœ
        
    Raises:
        æœ€åä¸€æ¬¡å¤±è´¥çš„å¼‚å¸¸
    """
    last_exception = None
    retry_count = StockPoolConfig.MAX_RETRY_COUNT
    func_name = getattr(func, '__name__', str(func))
    
    logger.info(f"å¼€å§‹æ‰§è¡Œå‡½æ•° {func_name}, æœ€å¤§é‡è¯•æ¬¡æ•°: {retry_count}")
    
    for i in range(retry_count):
        try:
            logger.debug(f"ç¬¬{i + 1}æ¬¡å°è¯•æ‰§è¡Œ {func_name}")
            result = func(*args, **kwargs)
            
            if i > 0:
                logger.info(f"å‡½æ•° {func_name} åœ¨ç¬¬{i + 1}æ¬¡å°è¯•åæˆåŠŸæ‰§è¡Œ")
            
            return result
            
        except Exception as e:
            last_exception = e
            if i < retry_count - 1:  # ä¸æ˜¯æœ€åä¸€æ¬¡å°è¯•
                wait_time = (i + 1) * StockPoolConfig.RETRY_BASE_DELAY
                logger.warning(f"å‡½æ•° {func_name} ç¬¬{i + 1}æ¬¡å°è¯•å¤±è´¥ï¼Œ{wait_time}ç§’åé‡è¯•: {e}")
                time.sleep(wait_time)
            else:
                logger.error(f"å‡½æ•° {func_name} é‡è¯•{retry_count}æ¬¡åä»å¤±è´¥: {e}")
    
    # æ‰€æœ‰é‡è¯•éƒ½å¤±è´¥äº†ï¼ŒæŠ›å‡ºæœ€åçš„å¼‚å¸¸
    raise last_exception
    

def get_stock_pool(selected=False):
    """
    è·å–è‚¡ç¥¨æ± æ•°æ®ï¼Œé€šè¿‡å¤šä¸ªæ—¶é—´åŒºé—´ç»„åˆè®¡ç®—é‡è¦åº¦
    
    Args:
        selected: æ˜¯å¦ä½¿ç”¨ç­›é€‰æ¡ä»¶ï¼ˆéSTã€éé€€å¸‚ç­‰ï¼‰
    
    Returns:
        æŒ‰é‡è¦åº¦æ’åºçš„è‚¡ç¥¨æ± DataFrame
    """
    try:
        all_df = []
        # ä½¿ç”¨é…ç½®ç±»ä¸­çš„åŒºé—´è®¾ç½®
        for days, rank in StockPoolConfig.INTERVAL_CONFIGS:
            logger.info(f'========== selected: {selected}  {days}-{rank} ===========')
            df = execute_with_retry(get_top_stocks, days=days, rank=rank, use_filters=selected)
            if df is None or df.empty:
                raise Exception(f'{(days,rank)}è·å–æ•°æ®å¤±è´¥')
            all_df.append(df)
            time.sleep(StockPoolConfig.QUERY_SLEEP_SECONDS)
        
        df_all = pd.concat(all_df)
        df = calc_importance(df_all, 
                           alpha=StockPoolConfig.IMPORTANCE_ALPHA, 
                           beta=StockPoolConfig.IMPORTANCE_BETA)
        return df
    except Exception as e:
        logger.error(f"è·å–è‚¡ç¥¨æ± å¤±è´¥: {e}")
        return None

def save_stock_pool_image(df: pd.DataFrame, date_str: str = None) -> str:
    """
    å°†è‚¡ç¥¨æ± ä¿å­˜ä¸ºè¡¨æ ¼å›¾ç‰‡
    
    Args:
        df: åŒ…å«è‚¡ç¥¨æ•°æ®çš„DataFrame
        date_str: æ—¥æœŸå­—ç¬¦ä¸²ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨å½“å‰æ—¥æœŸ
    
    Returns:
        ä¿å­˜çš„å›¾ç‰‡æ–‡ä»¶è·¯å¾„
    """
    if date_str is None:
        date_str = datetime.now().strftime('%Y%m%d')
    
    logger.info(f"å¼€å§‹ç”Ÿæˆè‚¡ç¥¨æ± å›¾ç‰‡, è‚¡ç¥¨æ•°é‡: {len(df)}")
    start_time = time.time()
    
    # è·å–è‚¡ç¥¨ç®€ç§°åˆ—è¡¨
    if 'è‚¡ç¥¨ç®€ç§°' not in df.columns:
        raise ValueError("DataFrameä¸­ç¼ºå°‘'è‚¡ç¥¨ç®€ç§°'åˆ—")
    
    stock_names = df['è‚¡ç¥¨ç®€ç§°'].tolist()
    
    # è®¾ç½®ä¸­æ–‡å­—ä½“
    plt.rcParams['font.sans-serif'] = ['SimHei', 'Arial Unicode MS', 'DejaVu Sans']
    plt.rcParams['axes.unicode_minus'] = False
    
    # åˆ›å»ºå›¾ç‰‡
    fig, ax = plt.subplots(figsize=(StockPoolConfig.IMAGE_WIDTH, StockPoolConfig.IMAGE_HEIGHT), 
                          dpi=StockPoolConfig.IMAGE_DPI)
    
    # éšè—åæ ‡è½´
    ax.set_xlim(0, 1)
    ax.set_ylim(0, len(stock_names))
    ax.axis('off')
    
    # ç»˜åˆ¶è¡¨æ ¼
    row_height = 1
    col_width = 1
    
    for i, stock_name in enumerate(stock_names):
        y_pos = len(stock_names) - i - 1  # ä»ä¸Šåˆ°ä¸‹æ’åˆ—
        
        # ç»˜åˆ¶çŸ©å½¢è¾¹æ¡†
        rect = patches.Rectangle((0, y_pos), col_width, row_height, 
                               linewidth=2, edgecolor='black', facecolor='white')
        ax.add_patch(rect)
        
        # æ·»åŠ è‚¡ç¥¨åç§°æ–‡æœ¬
        ax.text(0.5, y_pos + 0.5, stock_name, 
               ha='center', va='center', 
               fontsize=StockPoolConfig.FONT_SIZE, 
               color='black', weight='bold')
    
    # è°ƒæ•´å¸ƒå±€
    plt.tight_layout()
    plt.subplots_adjust(left=0.05, right=0.95, top=0.95, bottom=0.05)
    
    # ç¡®ä¿ä¿å­˜ç›®å½•å­˜åœ¨
    image_path = StockPoolConfig.get_image_save_path(date_str)
    image_dir = os.path.dirname(image_path)
    os.makedirs(image_dir, exist_ok=True)
    
    # ä¿å­˜å›¾ç‰‡
    plt.savefig(image_path, dpi=StockPoolConfig.IMAGE_DPI, 
               bbox_inches='tight', facecolor='white', edgecolor='none')
    plt.close()
    
    processing_time = time.time() - start_time
    logger.info(f"è‚¡ç¥¨æ± å›¾ç‰‡ç”Ÿæˆå®Œæˆ, è€—æ—¶: {processing_time:.2f}ç§’, ä¿å­˜è·¯å¾„: {image_path}")
    
    return image_path

def main():
    """
    ä¸»å‡½æ•°ï¼šè·å–è‚¡ç¥¨æ± æ•°æ®å¹¶ä¿å­˜
    
    æ‰§è¡Œæµç¨‹ï¼š
    1. æ£€æŸ¥æ˜¯å¦ä¸ºäº¤æ˜“æ—¥
    2. è·å–å…¨é‡è‚¡ç¥¨æ± æ•°æ®ï¼ˆæ— ç­›é€‰ï¼‰
    3. è·å–ç­›é€‰åçš„è‚¡ç¥¨æ± æ•°æ®ï¼ˆæœ‰ç­›é€‰æ¡ä»¶ï¼‰  
    4. åˆå¹¶æ•°æ®å¹¶å»é‡ï¼ˆåŒä¸€è‚¡ç¥¨ä¿ç•™é‡è¦åº¦æœ€é«˜çš„è®°å½•ï¼‰
    5. ä¿å­˜ç»“æœå¹¶å‘é€é€šçŸ¥
    """
    logger.info("å¼€å§‹æ‰§è¡Œè‚¡ç¥¨æ± æ•°æ®è·å–ä»»åŠ¡...")
    
    # æ£€æŸ¥æ˜¯å¦ä¸ºäº¤æ˜“æ—¥
    today = datetime.now().strftime('%Y%m%d')
    if not trading_calendar.is_trading_day(today):
        logger.info(f"ä»Šæ—¥({today})éäº¤æ˜“æ—¥ï¼Œè·³è¿‡æ‰§è¡Œ")
        return
    
    for i in range(StockPoolConfig.MAX_RETRY_COUNT):
        try:
            # è·å–å…¨é‡æ•°æ®ï¼ˆæ— ç­›é€‰æ¡ä»¶ï¼‰
            logger.info("æ­¥éª¤1: è·å–å…¨é‡è‚¡ç¥¨æ± æ•°æ®")
            full_data = get_stock_pool(selected=False)
            if full_data is None or full_data.empty:
                logger.warning("å…¨é‡æ•°æ®è·å–å¤±è´¥ï¼Œè·³è¿‡æœ¬æ¬¡å°è¯•")
                continue
            logger.info(f'å…¨é‡æ•°æ®è·å–æˆåŠŸ, æ•°æ®é‡: {full_data.shape}')
            
            # è·å–ç­›é€‰æ•°æ®ï¼ˆæœ‰ç­›é€‰æ¡ä»¶ï¼‰
            logger.info("æ­¥éª¤2: è·å–ç­›é€‰åè‚¡ç¥¨æ± æ•°æ®")
            filtered_data = get_stock_pool(selected=True)
            if filtered_data is None or filtered_data.empty:
                logger.warning("ç­›é€‰æ•°æ®è·å–å¤±è´¥ï¼Œè·³è¿‡æœ¬æ¬¡å°è¯•")
                continue
            logger.info(f'ç­›é€‰æ•°æ®è·å–æˆåŠŸ, æ•°æ®é‡: {filtered_data.shape}')
            
            # åˆå¹¶å¹¶å»é‡å¤„ç†
            logger.info("æ­¥éª¤3: åˆå¹¶æ•°æ®å¹¶å»é‡")
            combined_df = pd.concat([full_data, filtered_data], ignore_index=True)
            
            # æŒ‰è‚¡ç¥¨åˆ†ç»„ï¼Œä¿ç•™é‡è¦åº¦æœ€é«˜çš„è®°å½•
            groupby_keys = ['äº¤æ˜“æ—¥æœŸ', 'è‚¡ç¥¨ç®€ç§°', 'market_code', 'code']
            max_importance_idx = combined_df.groupby(groupby_keys)['é‡è¦åº¦'].idxmax()
            final_df = (combined_df.loc[max_importance_idx]
                       .sort_values(by='é‡è¦åº¦', ascending=False)
                       .reset_index(drop=True))
            
            # æ­¥éª¤4: æ•°æ®å¯¹æ¯”åˆ†æ
            logger.info("æ­¥éª¤4: ä¸å‰ä¸€äº¤æ˜“æ—¥æ•°æ®è¿›è¡Œå¯¹æ¯”åˆ†æ")
            comparison_msg = compare_with_previous_trading_day(final_df)
            
            # æ­¥éª¤5: ä¿å­˜æ•°æ®
            logger.info("æ­¥éª¤5: ä¿å­˜æ•°æ®")
            save_path = StockPoolConfig.get_csv_save_path()
            # åˆ›å»ºæ‰€éœ€çš„ç›®å½•ç»“æ„
            os.makedirs(f"{StockPoolConfig.DATA_SAVE_DIR}/{StockPoolConfig.CSV_SUBDIR}", exist_ok=True)
            os.makedirs(f"{StockPoolConfig.DATA_SAVE_DIR}/{StockPoolConfig.IMAGES_SUBDIR}", exist_ok=True)
            final_df.to_csv(save_path, index=False)

            # æ­¥éª¤6: ä¿å­˜è‚¡ç¥¨æ± å›¾ç‰‡
            logger.info("æ­¥éª¤6: ä¿å­˜è‚¡ç¥¨æ± å›¾ç‰‡")
            save_stock_pool_image(final_df)
            
            # æ­¥éª¤7: å‘é€å¯¹æ¯”ç»“æœé€šçŸ¥
            logger.info("æ­¥éª¤7: å‘é€æ•°æ®å¯¹æ¯”ç»“æœåˆ°é’‰é’‰")
            logger.info(f"ä»»åŠ¡å®Œæˆ! æœ€ç»ˆæ•°æ®é‡: {final_df.shape}, ä¿å­˜è·¯å¾„: {save_path}")
            send_dingding_msg(comparison_msg)
            return  # æˆåŠŸå®Œæˆï¼Œé€€å‡ºå‡½æ•°
            
        except Exception as e:
            if i == StockPoolConfig.MAX_RETRY_COUNT - 1:
                # æœ€åä¸€æ¬¡å°è¯•å¤±è´¥
                error_msg = f"æ¯æ—¥å¤ç›˜ \n ä»Šæ—¥è‚¡ç¥¨æ± æ›´æ–°å¤±è´¥: {e}"
                logger.error(f"é‡è¯•{StockPoolConfig.MAX_RETRY_COUNT}æ¬¡åä»å¤±è´¥: {e}")
                send_dingding_msg(error_msg)
            else:
                # è¿˜æœ‰é‡è¯•æœºä¼š
                logger.warning(f"ç¬¬{i+1}æ¬¡å°è¯•å¤±è´¥ï¼Œå‡†å¤‡é‡è¯•: {e}")
            
            if i < StockPoolConfig.MAX_RETRY_COUNT - 1:
                time.sleep(StockPoolConfig.MAIN_RETRY_SLEEP_SECONDS)

if __name__ == "__main__":
    main()