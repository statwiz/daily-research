import pywencai
import re
import pandas as pd
from typing import Optional
import numpy as np
import time
import os
from datetime import datetime, timedelta
from log_setup import get_logger
from utils import send_dingding_msg, TradingCalendar
from wencai_utils import WencaiUtils
import akshare as ak
import configparser

import warnings
warnings.filterwarnings("ignore")

trading_calendar = TradingCalendar()

# é…ç½®ç±» - ç»Ÿä¸€ç®¡ç†æ‰€æœ‰å‚æ•°
class StockPoolConfig:
    """è‚¡ç¥¨æ± é…ç½®ç±»"""
    # é‡è¯•é…ç½®
    MAX_RETRY_COUNT = 3
    RETRY_BASE_DELAY = 2  # åŸºç¡€ç­‰å¾…æ—¶é—´ï¼Œå®é™…ç­‰å¾…æ—¶é—´ä¸º (attempt + 1) * BASE_DELAY
    
    # åŒºé—´é…ç½® [(å¤©æ•°, æ’å)]
    INTERVAL_CONFIGS = [
        (2, 10),   # æœ€è¿‘2æ—¥å‰10å
        (3, 10),   # æœ€è¿‘3æ—¥å‰10å  
        (5, 10),   # æœ€è¿‘5æ—¥å‰10å
        (10, 10),   # æœ€è¿‘10æ—¥å‰10å
        (15, 5),   # æœ€è¿‘15æ—¥å‰5å
    ]
    
    # é‡è¦åº¦è®¡ç®—å‚æ•°
    IMPORTANCE_ALPHA = 0.99  # æ’åæƒé‡ï¼Œè¶Šå¤§è¶Šå¼ºè°ƒæ’åå·®è·
    IMPORTANCE_BETA = 0.01   # åŒºé—´é•¿åº¦æƒé‡ï¼Œè¶Šå¤§è¶Šå¼ºè°ƒçŸ­åŒºé—´
    
    # æ•°æ®ä¿å­˜é…ç½®
    DATA_SAVE_DIR = 'data'
    CSV_SUBDIR = 'csv'
    TXT_SUBDIR = 'txt'
    DATA_FILE_PREFIX = 'stock_pool'
    
    @classmethod
    def get_csv_save_path(cls, date_str: str = None) -> str:
        """ç”ŸæˆCSVæ–‡ä»¶ä¿å­˜è·¯å¾„"""
        if date_str is None:
            date_str = datetime.now().strftime('%Y%m%d')
        return f"{cls.DATA_SAVE_DIR}/{cls.CSV_SUBDIR}/{cls.DATA_FILE_PREFIX}_{date_str}.csv"
    
    @classmethod
    def get_txt_save_path(cls, date_str: str = None) -> str:
        """ç”ŸæˆTXTæ–‡ä»¶ä¿å­˜è·¯å¾„"""
        if date_str is None:
            date_str = datetime.now().strftime('%Y%m%d')
        return f"{cls.DATA_SAVE_DIR}/{cls.TXT_SUBDIR}/{cls.DATA_FILE_PREFIX}_{date_str}.txt"
    
    @classmethod 
    def get_data_save_path(cls, date_str: str = None) -> str:
        """è·å–ä¸»è¦æ•°æ®ä¿å­˜è·¯å¾„"""
        return cls.get_csv_save_path(date_str)
    
    @classmethod
    def get_zj_csv_save_path(cls, date_str: str = None) -> str:
        """ç”Ÿæˆèµ„é‡‘ç‰ˆCSVæ–‡ä»¶ä¿å­˜è·¯å¾„ï¼ˆå¤§äº200äº¿æµé€šå¸‚å€¼ï¼‰"""
        if date_str is None:
            date_str = datetime.now().strftime('%Y%m%d')
        return f"{cls.DATA_SAVE_DIR}/{cls.CSV_SUBDIR}/{cls.DATA_FILE_PREFIX}_zj_{date_str}.csv"
    
    @classmethod
    def get_zj_txt_save_path(cls, date_str: str = None) -> str:
        """ç”Ÿæˆèµ„é‡‘ç‰ˆTXTæ–‡ä»¶ä¿å­˜è·¯å¾„ï¼ˆå¤§äº200äº¿æµé€šå¸‚å€¼ï¼‰"""
        if date_str is None:
            date_str = datetime.now().strftime('%Y%m%d')
        return f"{cls.DATA_SAVE_DIR}/{cls.TXT_SUBDIR}/{cls.DATA_FILE_PREFIX}_zj_{date_str}.txt"
    
    # æŸ¥è¯¢ç­‰å¾…æ—¶é—´
    QUERY_SLEEP_SECONDS = 3
    MAIN_RETRY_SLEEP_SECONDS = 10

# æ—¥å¿—é…ç½®
logger = get_logger("stock_pool", "logs", "stock_pool.log")




def compare_with_previous_trading_day(current_df: pd.DataFrame, date_str: str = None) -> str:
    """ä¸å‰ä¸€ä¸ªäº¤æ˜“æ—¥çš„æ•°æ®è¿›è¡Œå¯¹æ¯”ï¼Œè¿”å›å¯¹æ¯”æ¶ˆæ¯"""
    if date_str is None:
        date_str = datetime.now().strftime('%Y%m%d')
    
    # è·å–å‰ä¸€äº¤æ˜“æ—¥æ•°æ®
    previous_date = trading_calendar.get_previous_trading_day(date_str)
    if not previous_date:
        return f"ğŸ“Š ä»Šæ—¥è‚¡ç¥¨æ± æ›´æ–°å®Œæˆ\næ•°æ®é‡: {len(current_df)}åªè‚¡ç¥¨\nâš ï¸ æœªæ‰¾åˆ°å‰ä¸€äº¤æ˜“æ—¥æ•°æ®"
    
    # æŸ¥æ‰¾å‰ä¸€äº¤æ˜“æ—¥çš„CSVæ–‡ä»¶
    previous_date_str = previous_date.strftime('%Y%m%d')
    previous_file = StockPoolConfig.get_csv_save_path(previous_date_str)
    
    if not os.path.exists(previous_file):
        return f"ğŸ“Š ä»Šæ—¥è‚¡ç¥¨æ± æ›´æ–°å®Œæˆ\næ•°æ®é‡: {len(current_df)}åªè‚¡ç¥¨\nâš ï¸ æœªæ‰¾åˆ°å‰ä¸€äº¤æ˜“æ—¥æ•°æ®æ–‡ä»¶"
    
    try:
        # è¯»å–CSVæ–‡ä»¶ï¼ŒæŒ‡å®šè‚¡ç¥¨ä»£ç åˆ—ä¸ºå­—ç¬¦ä¸²ç±»å‹ä»¥ä¿æŒå‰å¯¼é›¶
        previous_df = pd.read_csv(previous_file, dtype={'code': str, 'market_code': str})
    except Exception as e:
        logger.warning(f"è¯»å–å‰ä¸€äº¤æ˜“æ—¥æ•°æ®å¤±è´¥: {e}")
        return f"ğŸ“Š ä»Šæ—¥è‚¡ç¥¨æ± æ›´æ–°å®Œæˆ\næ•°æ®é‡: {len(current_df)}åªè‚¡ç¥¨\nâš ï¸ è¯»å–å‰ä¸€äº¤æ˜“æ—¥æ•°æ®å¤±è´¥"
    
    # åŸºäºè‚¡ç¥¨ä»£ç è®¡ç®—å˜åŠ¨ï¼ˆç¡®ä¿å”¯ä¸€æ€§ï¼‰
    current_codes = set(current_df['code'].astype(str))
    previous_codes = set(previous_df['code'].astype(str))
    
    # è·å–æ–°å¢å’Œç§»é™¤çš„è‚¡ç¥¨ä»£ç 
    new_codes = current_codes - previous_codes
    removed_codes = previous_codes - current_codes
    
    # æ„å»ºåŸºç¡€æ¶ˆæ¯
    date_fmt = f"{previous_date.strftime('%Y-%m-%d')}"
    message = [
        "ğŸ“Š æ¯æ—¥å¤ç›˜:è‚¡ç¥¨æ± ",
        f"ğŸ“… å¯¹æ¯”åŸºå‡†: {date_fmt}",
        f"ğŸ“ˆ ä»Šæ—¥: {len(current_codes)}åª | æ˜¨æ—¥: {len(previous_codes)}åª"
    ]
    
    if len(new_codes) == 0 and len(removed_codes) == 0:
        message.append("âœ¨ è‚¡ç¥¨æ± æ— å˜åŠ¨")
    else:
        # æ–°å¢è‚¡ç¥¨è¯¦æƒ…
        if len(new_codes) > 0:
            message.append(f"ğŸ†• æ–°å¢: {len(new_codes)}åª")
            
            # è·å–æ–°å¢è‚¡ç¥¨çš„è¯¦ç»†ä¿¡æ¯
            new_stocks_info = current_df[current_df['code'].astype(str).isin(new_codes)].copy()
            new_stocks_info = new_stocks_info.sort_values('é‡è¦åº¦', ascending=False)
            
            for _, stock in new_stocks_info.iterrows():
                code = stock['code']
                name = stock['è‚¡ç¥¨ç®€ç§°']
                interval_info = stock.get('åŒºé—´ä¿¡æ¯', 'æ— ')
                message.append(f"  â€¢ {name}({code}) åŒºé—´:{interval_info}")
        
        # ç§»é™¤è‚¡ç¥¨ç»Ÿè®¡
        if len(removed_codes) > 0:
            message.append(f"ğŸ”» ç§»é™¤: {len(removed_codes)}åª")
            
            # è·å–ç§»é™¤è‚¡ç¥¨çš„è¯¦ç»†ä¿¡æ¯ï¼ˆä»å‰ä¸€å¤©çš„æ•°æ®ä¸­ï¼‰
            removed_stocks_info = previous_df[previous_df['code'].astype(str).isin(removed_codes)].copy()
            removed_stocks_info = removed_stocks_info.sort_values('é‡è¦åº¦', ascending=False)
            
            for _, stock in removed_stocks_info.iterrows():
                code = stock['code']
                name = stock['è‚¡ç¥¨ç®€ç§°']
                interval_info = stock.get('åŒºé—´ä¿¡æ¯', 'æ— ')
                message.append(f"  â€¢ {name}({code}) åŒºé—´:{interval_info}")
    
    msg = "\n".join(message)
    logger.info(msg)
    return msg



def calc_importance(df: pd.DataFrame, 
                   alpha: float = None, 
                   beta: float = None) -> pd.DataFrame:
    """
    åŸºäºåŸå§‹æ•°æ®èšåˆå¹¶è®¡ç®—è‚¡ç¥¨é‡è¦åº¦
    
    Args:
        df: åŒ…å«è‚¡ç¥¨åŒºé—´æ•°æ®çš„DataFrame
        alpha: æ’åæƒé‡å‚æ•°ï¼Œè¶Šå¤§è¶Šå¼ºè°ƒæ’åå·®è·
        beta: åŒºé—´é•¿åº¦æƒé‡å‚æ•°ï¼Œè¶Šå¤§è¶Šå¼ºè°ƒçŸ­åŒºé—´
    
    Returns:
        æŒ‰é‡è¦åº¦æ’åºçš„DataFrameï¼ŒåŒ…å«åŒºé—´ä¿¡æ¯å’Œé‡è¦åº¦å¾—åˆ†
        
    è®¡ç®—é€»è¾‘:
        1. åˆ†ç»„å­—æ®µ: [äº¤æ˜“æ—¥æœŸ, è‚¡ç¥¨ç®€ç§°, market_code, code]
        2. åŒºé—´ä¿¡æ¯: 'åŒºé—´é•¿åº¦-åŒºé—´æ’å' ç”¨ | è¿æ¥
        3. é‡è¦åº¦: 100 * sum(exp(-alpha * æ’å) * exp(-beta * åŒºé—´é•¿åº¦))
        4. æ’åº: æŒ‰é‡è¦åº¦é™åº
    """
    # ä½¿ç”¨é…ç½®é»˜è®¤å€¼
    if alpha is None:
        alpha = StockPoolConfig.IMPORTANCE_ALPHA
    if beta is None:
        beta = StockPoolConfig.IMPORTANCE_BETA
    
    logger.info(f"å¼€å§‹è®¡ç®—è‚¡ç¥¨é‡è¦åº¦, æ•°æ®é‡: {len(df)}, alpha: {alpha}, beta: {beta}")
    start_time = time.time()
    
    def aggregate_importance(sub_df):
        """èšåˆå‡½æ•°ï¼šè®¡ç®—å•åªè‚¡ç¥¨çš„é‡è¦åº¦"""
        interval_info = "|".join(
            f"{length}-{rank}" for length, rank in zip(sub_df["åŒºé—´é•¿åº¦"], sub_df["åŒºé—´æ’å"])
        )
        # é‡è¦åº¦è®¡ç®—ï¼šä½¿ç”¨å¯¹æ•°å‡½æ•°
        importance_score = float(
            100 * (1 / (np.log1p(sub_df["åŒºé—´æ’å"]) * np.log1p(sub_df["åŒºé—´é•¿åº¦"]))).sum()
        )

        return pd.Series({
            "åŒºé—´ä¿¡æ¯": interval_info,
            "é‡è¦åº¦": importance_score
        })

    # æŒ‰è‚¡ç¥¨åˆ†ç»„å¹¶è®¡ç®—é‡è¦åº¦
    grouped_df = (
        df.groupby(["äº¤æ˜“æ—¥æœŸ", "è‚¡ç¥¨ç®€ç§°", "å¸‚å€¼Z", "market_code", "code"], as_index=False)
        .apply(aggregate_importance)
    )

    # æŒ‰é‡è¦åº¦é™åºæ’åº
    grouped_df.sort_values(by="é‡è¦åº¦", ascending=False, inplace=True)
    grouped_df.reset_index(drop=True, inplace=True)
    
    processing_time = time.time() - start_time
    logger.info(f"é‡è¦åº¦è®¡ç®—å®Œæˆ, è€—æ—¶: {processing_time:.2f}ç§’, ç»“æœæ•°é‡: {len(grouped_df)}")
    
    return grouped_df[["äº¤æ˜“æ—¥æœŸ", "è‚¡ç¥¨ç®€ç§°", "å¸‚å€¼Z", "market_code", "code", "åŒºé—´ä¿¡æ¯", "é‡è¦åº¦"]]



def execute_with_retry(func, *args, **kwargs):
    """
    å¸¦é‡è¯•æœºåˆ¶çš„å‡½æ•°åŒ…è£…å™¨
    
    Args:
        func: è¦æ‰§è¡Œçš„å‡½æ•°
        *args, **kwargs: å‡½æ•°å‚æ•°
    
    Returns:
        å‡½æ•°æ‰§è¡Œç»“æœ
        
    Raises:
        æœ€åä¸€æ¬¡å¤±è´¥çš„å¼‚å¸¸
    """
    last_exception = None
    retry_count = StockPoolConfig.MAX_RETRY_COUNT
    func_name = getattr(func, '__name__', str(func))
    
    logger.info(f"å¼€å§‹æ‰§è¡Œå‡½æ•° {func_name}, æœ€å¤§é‡è¯•æ¬¡æ•°: {retry_count}")
    
    for i in range(retry_count):
        try:
            logger.debug(f"ç¬¬{i + 1}æ¬¡å°è¯•æ‰§è¡Œ {func_name}")
            result = func(*args, **kwargs)
            
            if i > 0:
                logger.info(f"å‡½æ•° {func_name} åœ¨ç¬¬{i + 1}æ¬¡å°è¯•åæˆåŠŸæ‰§è¡Œ")
            
            return result
            
        except Exception as e:
            last_exception = e
            if i < retry_count - 1:  # ä¸æ˜¯æœ€åä¸€æ¬¡å°è¯•
                wait_time = (i + 1) * StockPoolConfig.RETRY_BASE_DELAY
                logger.warning(f"å‡½æ•° {func_name} ç¬¬{i + 1}æ¬¡å°è¯•å¤±è´¥ï¼Œ{wait_time}ç§’åé‡è¯•: {e}")
                time.sleep(wait_time)
            else:
                logger.error(f"å‡½æ•° {func_name} é‡è¯•{retry_count}æ¬¡åä»å¤±è´¥: {e}")
    
    # æ‰€æœ‰é‡è¯•éƒ½å¤±è´¥äº†ï¼ŒæŠ›å‡ºæœ€åçš„å¼‚å¸¸
    raise last_exception
    

def get_stock_pool(selected=None):
    """
    è·å–è‚¡ç¥¨æ± æ•°æ®ï¼Œé€šè¿‡å¤šä¸ªæ—¶é—´åŒºé—´ç»„åˆè®¡ç®—é‡è¦åº¦
    
    Args:
        selected: æ˜¯å¦ä½¿ç”¨ç­›é€‰æ¡ä»¶ï¼ˆéSTã€éé€€å¸‚ç­‰ï¼‰
    
    Returns:
        æŒ‰é‡è¦åº¦æ’åºçš„è‚¡ç¥¨æ± DataFrame
    """
    try:
        all_df = []
        # ä½¿ç”¨é…ç½®ç±»ä¸­çš„åŒºé—´è®¾ç½®
        for days, rank in StockPoolConfig.INTERVAL_CONFIGS:
            logger.info(f'========== selected: {selected}  {days}-{rank} ===========')
            df = execute_with_retry(WencaiUtils.get_top_stocks, days=days, rank=rank, use_filters=selected)
            if df is None or df.empty:
                raise Exception(f'{(days,rank)}è·å–æ•°æ®å¤±è´¥')
            all_df.append(df)
            time.sleep(StockPoolConfig.QUERY_SLEEP_SECONDS)
        
        df_all = pd.concat(all_df)
        df = calc_importance(df_all, 
                           alpha=StockPoolConfig.IMPORTANCE_ALPHA, 
                           beta=StockPoolConfig.IMPORTANCE_BETA)
        return df
    except Exception as e:
        logger.error(f"è·å–è‚¡ç¥¨æ± å¤±è´¥: {e}")
        return None

def save_stock_pool_codes(df: pd.DataFrame, date_str: str = None) -> str:
    """
    å°†è‚¡ç¥¨æ± çš„ä»£ç ä¿å­˜ä¸ºtxtæ–‡ä»¶
    
    Args:
        df: åŒ…å«è‚¡ç¥¨æ•°æ®çš„DataFrame
        date_str: æ—¥æœŸå­—ç¬¦ä¸²ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨å½“å‰æ—¥æœŸ
    
    Returns:
        ä¿å­˜çš„txtæ–‡ä»¶è·¯å¾„
    """
    if date_str is None:
        date_str = datetime.now().strftime('%Y%m%d')
    
    logger.info(f"å¼€å§‹ä¿å­˜è‚¡ç¥¨æ± ä»£ç åˆ°txtæ–‡ä»¶, è‚¡ç¥¨æ•°é‡: {len(df)}")
    start_time = time.time()
    
    # æ£€æŸ¥å¿…è¦çš„åˆ—
    if df.empty or 'code' not in df.columns:
        raise ValueError("DataFrameä¸­ç¼ºå°‘'code'åˆ—")
    
    # è·å–è‚¡ç¥¨ä»£ç åˆ—è¡¨ï¼Œå¹¶ç¡®ä¿æ ¼å¼æ­£ç¡®ï¼ˆ6ä½æ•°å­—ï¼‰
    stock_codes = df['code'].astype(str).str.zfill(6).tolist()
    
    # ç¡®ä¿ä¿å­˜ç›®å½•å­˜åœ¨
    txt_path = StockPoolConfig.get_txt_save_path(date_str)
    txt_dir = os.path.dirname(txt_path)
    os.makedirs(txt_dir, exist_ok=True)
    
    # ä¿å­˜ä»£ç åˆ°txtæ–‡ä»¶ï¼Œæ¯è¡Œä¸€ä¸ªä»£ç 
    with open(txt_path, 'w', encoding='utf-8') as f:
        for code in stock_codes:
            f.write(f"{code}\n")
    
    processing_time = time.time() - start_time
    logger.info(f"è‚¡ç¥¨æ± ä»£ç ä¿å­˜å®Œæˆ, è€—æ—¶: {processing_time:.2f}ç§’, ä¿å­˜è·¯å¾„: {txt_path}")
    
    return txt_path

def save_zj_stock_pool_codes(df: pd.DataFrame, date_str: str = None) -> str:
    """
    å°†èµ„é‡‘ç‰ˆè‚¡ç¥¨æ± çš„ä»£ç ä¿å­˜ä¸ºtxtæ–‡ä»¶ï¼ˆå¤§äº200äº¿æµé€šå¸‚å€¼ï¼‰
    
    Args:
        df: åŒ…å«è‚¡ç¥¨æ•°æ®çš„DataFrame
        date_str: æ—¥æœŸå­—ç¬¦ä¸²ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨å½“å‰æ—¥æœŸ
    
    Returns:
        ä¿å­˜çš„txtæ–‡ä»¶è·¯å¾„
    """
    if date_str is None:
        date_str = datetime.now().strftime('%Y%m%d')
    
    logger.info(f"å¼€å§‹ä¿å­˜ä¸­å†›ç‰ˆè‚¡ç¥¨æ± ä»£ç åˆ°txtæ–‡ä»¶, è‚¡ç¥¨æ•°é‡: {len(df)}")
    start_time = time.time()
    
    # æ£€æŸ¥å¿…è¦çš„åˆ—
    if df.empty or 'code' not in df.columns:
        raise ValueError("DataFrameä¸­ç¼ºå°‘'code'åˆ—")
    
    # è·å–è‚¡ç¥¨ä»£ç åˆ—è¡¨ï¼Œå¹¶ç¡®ä¿æ ¼å¼æ­£ç¡®ï¼ˆ6ä½æ•°å­—ï¼‰
    stock_codes = df['code'].astype(str).str.zfill(6).tolist()
    
    # ç¡®ä¿ä¿å­˜ç›®å½•å­˜åœ¨
    txt_path = StockPoolConfig.get_zj_txt_save_path(date_str)
    txt_dir = os.path.dirname(txt_path)
    os.makedirs(txt_dir, exist_ok=True)
    
    # ä¿å­˜ä»£ç åˆ°txtæ–‡ä»¶ï¼Œæ¯è¡Œä¸€ä¸ªä»£ç 
    with open(txt_path, 'w', encoding='utf-8') as f:
        for code in stock_codes:
            f.write(f"{code}\n")
    
    processing_time = time.time() - start_time
    logger.info(f"ä¸­å†›ç‰ˆè‚¡ç¥¨æ± ä»£ç ä¿å­˜å®Œæˆ, è€—æ—¶: {processing_time:.2f}ç§’, ä¿å­˜è·¯å¾„: {txt_path}")
    
    return txt_path

def collect_stock_pool_data() -> list:
    """
    æ”¶é›†æ‰€æœ‰è‚¡ç¥¨æ± æ•°æ®
    
    Returns:
        åŒ…å«æ‰€æœ‰è‚¡ç¥¨æ± DataFrameçš„åˆ—è¡¨
    """
    data_list = []
    
    # è‚¡ç¥¨æ± é…ç½®ï¼š(ç­›é€‰æ¡ä»¶, æè¿°)
    pool_configs = [
        (None, "å…¨é‡è‚¡ç¥¨æ± æ•°æ®"),
        ('30', "ç­›é€‰åè‚¡ç¥¨æ± æ•°æ®: è‡ªç”±æµé€šå¸‚å€¼>30äº¿"),  
        ('60', "ç­›é€‰åè‚¡ç¥¨æ± æ•°æ®: è‡ªç”±æµé€šå¸‚å€¼>60äº¿"),
        ('100', "ç­›é€‰åè‚¡ç¥¨æ± æ•°æ®: è‡ªç”±æµé€šå¸‚å€¼>100äº¿"),
        ('200', "ç­›é€‰åè‚¡ç¥¨æ± æ•°æ®: è‡ªç”±æµé€šå¸‚å€¼>200äº¿"),
    ]
    
    for i, (selected, description) in enumerate(pool_configs, 1):
        step_name = "æ­¥éª¤1" if selected is None else f"æ­¥éª¤2.{i-1}"
        logger.info(f" ------------ {step_name}: è·å–{description} ------------")
        
        data = get_stock_pool(selected=selected)
        if data is None or data.empty:
            logger.warning(f"{description}è·å–å¤±è´¥ï¼Œè·³è¿‡æœ¬æ¬¡å°è¯•")
            return None  # ä»»ä½•ä¸€ä¸ªè·å–å¤±è´¥å°±è¿”å›None
        
        logger.info(f'{description}è·å–æˆåŠŸ, æ•°æ®é‡: {data.shape}')
        data_list.append(data)
    
    return data_list


def process_and_merge_data(data_list: list) -> pd.DataFrame:
    """
    åˆå¹¶å¹¶å»é‡è‚¡ç¥¨æ± æ•°æ®
    
    Args:
        data_list: åŒ…å«å¤šä¸ªè‚¡ç¥¨æ± DataFrameçš„åˆ—è¡¨
    
    Returns:
        å»é‡åæŒ‰é‡è¦åº¦æ’åºçš„DataFrame
    """    
    # åˆå¹¶æ‰€æœ‰æ•°æ®
    combined_df = pd.concat(data_list, ignore_index=True)
    
    # æŒ‰è‚¡ç¥¨åˆ†ç»„ï¼Œä¿ç•™é‡è¦åº¦æœ€é«˜çš„è®°å½•
    groupby_keys = ['äº¤æ˜“æ—¥æœŸ', 'è‚¡ç¥¨ç®€ç§°', 'å¸‚å€¼Z', 'market_code', 'code']
    max_importance_idx = combined_df.groupby(groupby_keys)['é‡è¦åº¦'].idxmax()
    final_df = (combined_df.loc[max_importance_idx]
               .sort_values(by='é‡è¦åº¦', ascending=False)
               .reset_index(drop=True))
    final_df['é‡è¦åº¦'] = final_df['é‡è¦åº¦'].round(2)
    
    return final_df


def save_stock_pool_data(final_df: pd.DataFrame, trade_date: str):
    """
    ä¿å­˜è‚¡ç¥¨æ± æ•°æ®åˆ°CSVå’ŒTXTæ–‡ä»¶
    
    Args:
        final_df: æœ€ç»ˆçš„è‚¡ç¥¨æ± æ•°æ®
        trade_date: äº¤æ˜“æ—¥æœŸ
    """    
    # åˆ›å»ºæ‰€éœ€çš„ç›®å½•ç»“æ„
    os.makedirs(f"{StockPoolConfig.DATA_SAVE_DIR}/{StockPoolConfig.CSV_SUBDIR}", exist_ok=True)
    os.makedirs(f"{StockPoolConfig.DATA_SAVE_DIR}/{StockPoolConfig.TXT_SUBDIR}", exist_ok=True)
    
    # ä¿å­˜ä¸»è¦è‚¡ç¥¨æ± æ•°æ®
    if len(final_df) > 0:
        save_path = StockPoolConfig.get_csv_save_path(trade_date)
        final_df.to_csv(save_path, index=False, encoding='utf-8-sig')
        logger.info(f"ä¸»è‚¡ç¥¨æ± CSVä¿å­˜å®Œæˆ, è·¯å¾„: {save_path}")
    else:
        logger.warning("æ²¡æœ‰æ‰¾åˆ°è‚¡ç¥¨ï¼Œè·³è¿‡æ•°æ®ä¿å­˜")
        return
    
    # ç­›é€‰å¤§äº200äº¿è‡ªç”±æµé€šå¸‚å€¼çš„è‚¡ç¥¨ï¼ˆèµ„é‡‘ç‰ˆï¼‰
    final_zj_df = final_df[final_df['å¸‚å€¼Z'] > 100 * 1e8]
    logger.info(f"ç­›é€‰å‡ºå¤§äº100äº¿è‡ªç”±æµé€šå¸‚å€¼çš„è‚¡ç¥¨æ•°é‡: {len(final_zj_df)}")
    
    # ä¿å­˜èµ„é‡‘ç‰ˆæ•°æ®
    if len(final_zj_df) > 0:
        zj_csv_path = StockPoolConfig.get_zj_csv_save_path(trade_date)
        final_zj_df.to_csv(zj_csv_path, index=False, encoding='utf-8-sig')
        logger.info(f"ä¸­å†›ç‰ˆè‚¡ç¥¨æ± CSVä¿å­˜å®Œæˆ, è·¯å¾„: {zj_csv_path}")
        
        # ä¿å­˜èµ„é‡‘ç‰ˆè‚¡ç¥¨ä»£ç åˆ°txtæ–‡ä»¶
        save_zj_stock_pool_codes(final_zj_df, trade_date)
    else:
        logger.warning("æ²¡æœ‰æ‰¾åˆ°å¤§äº100äº¿è‡ªç”±æµé€šå¸‚å€¼çš„è‚¡ç¥¨ï¼Œè·³è¿‡èµ„é‡‘ç‰ˆæ•°æ®ä¿å­˜")
    
    # ä¿å­˜è‚¡ç¥¨æ± ä»£ç åˆ°txtæ–‡ä»¶
    logger.info("ä¿å­˜è‚¡ç¥¨æ± ä»£ç åˆ°txtæ–‡ä»¶")
    save_stock_pool_codes(final_df, trade_date)


def main():
    """
    ä¸»å‡½æ•°ï¼šè·å–è‚¡ç¥¨æ± æ•°æ®å¹¶ä¿å­˜
    
    æ‰§è¡Œæµç¨‹ï¼š
    1. æ£€æŸ¥æ˜¯å¦ä¸ºäº¤æ˜“æ—¥
    2. æ”¶é›†æ‰€æœ‰è‚¡ç¥¨æ± æ•°æ®ï¼ˆå…¨é‡+å„ç­›é€‰æ¡ä»¶ï¼‰
    3. åˆå¹¶æ•°æ®å¹¶å»é‡ï¼ˆåŒä¸€è‚¡ç¥¨ä¿ç•™é‡è¦åº¦æœ€é«˜çš„è®°å½•ï¼‰
    4. æ•°æ®å¯¹æ¯”åˆ†æ
    5. ä¿å­˜ç»“æœå¹¶å‘é€é€šçŸ¥
    """
    logger.info("å¼€å§‹æ‰§è¡Œè‚¡ç¥¨æ± æ•°æ®è·å–ä»»åŠ¡...")
    
    # æ£€æŸ¥æ˜¯å¦ä¸ºäº¤æ˜“æ—¥
    today = datetime.now().strftime('%Y%m%d')
    # if not trading_calendar.is_trading_day(today):
    #     logger.info(f"ä»Šæ—¥({today})éäº¤æ˜“æ—¥ï¼Œè·³è¿‡æ‰§è¡Œ")
    #     return
    
    for i in range(StockPoolConfig.MAX_RETRY_COUNT):
        try:
            # æ­¥éª¤1-2: æ”¶é›†æ‰€æœ‰è‚¡ç¥¨æ± æ•°æ®
            data_list = collect_stock_pool_data()
            if data_list is None:
                logger.warning("è‚¡ç¥¨æ± æ•°æ®æ”¶é›†å¤±è´¥ï¼Œè·³è¿‡æœ¬æ¬¡å°è¯•")
                continue
            
            # æ­¥éª¤3: åˆå¹¶å¹¶å»é‡å¤„ç†
            logger.info(" ------------ æ­¥éª¤3: åˆå¹¶æ•°æ®å¹¶å»é‡ ------------")
            final_df = process_and_merge_data(data_list)
            
            # æ­¥éª¤4: æ•°æ®å¯¹æ¯”åˆ†æ
            logger.info("------------ æ­¥éª¤4: ä¸å‰ä¸€äº¤æ˜“æ—¥æ•°æ®è¿›è¡Œå¯¹æ¯”åˆ†æ------------")
            trade_date = final_df['äº¤æ˜“æ—¥æœŸ'].iloc[0]
            comparison_msg = compare_with_previous_trading_day(final_df, trade_date)
            
            # æ­¥éª¤5-6: ä¿å­˜æ•°æ®
            logger.info("------------ æ­¥éª¤5: ä¿å­˜æ•°æ®------------")
            save_stock_pool_data(final_df, trade_date)
            
            # æ­¥éª¤6: å‘é€å¯¹æ¯”ç»“æœé€šçŸ¥
            logger.info("------------ æ­¥éª¤6: å‘é€æ•°æ®å¯¹æ¯”ç»“æœåˆ°é’‰é’‰------------")
            logger.info(f"ä»»åŠ¡å®Œæˆ! æœ€ç»ˆæ•°æ®é‡: {final_df.shape}")
            send_dingding_msg(comparison_msg)
            return  # æˆåŠŸå®Œæˆï¼Œé€€å‡ºå‡½æ•°
            
        except Exception as e:
            if i == StockPoolConfig.MAX_RETRY_COUNT - 1:
                # æœ€åä¸€æ¬¡å°è¯•å¤±è´¥
                error_msg = f"æ¯æ—¥å¤ç›˜ \n ä»Šæ—¥è‚¡ç¥¨æ± æ›´æ–°å¤±è´¥: {e}"
                logger.error(f"é‡è¯•{StockPoolConfig.MAX_RETRY_COUNT}æ¬¡åä»å¤±è´¥: {e}")
                send_dingding_msg(error_msg)
            else:
                # è¿˜æœ‰é‡è¯•æœºä¼š
                logger.warning(f"ç¬¬{i+1}æ¬¡å°è¯•å¤±è´¥ï¼Œå‡†å¤‡é‡è¯•: {e}")
            
            if i < StockPoolConfig.MAX_RETRY_COUNT - 1:
                time.sleep(StockPoolConfig.MAIN_RETRY_SLEEP_SECONDS)

if __name__ == "__main__":
    main()